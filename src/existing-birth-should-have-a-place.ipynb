{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vérifier les matchings des geo places\n",
    "- Vérifier l'algo sur les types des geo places\n",
    "- Geo type \"Département of France\"?\n",
    "- Vérifier l'ordre de gestion des types (['AdministrativeRegion', 'ArchitecturalStructure', 'Building',\n",
    "       'Castle', 'City', 'Country', 'Czech_lands',\n",
    "       'Departments_of_France', 'Gemeinde', 'Imperial city', 'Island',\n",
    "       'Location', 'Mountain', 'MountainRange', 'MusicalArtist',\n",
    "       'NaturalPlace', 'Organisation', 'Person', 'Place',\n",
    "       'PopulatedPlace', 'Region', 'Settlement', 'SoccerClub', 'Stadt',\n",
    "       'Town', 'Village', 'arr', 'line'])\n",
    "- Mettre à jour les places géographiques existantes (manuellement?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing births should have a place\n",
    "\n",
    "For more info, see the related ticket: https://github.com/geovistory/switzerland-and-beyond/issues/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'staging'\n",
    "pk_project = 153\n",
    "execute = False\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "import geovpylib.database as db\n",
    "import geovpylib.utils as u\n",
    "import geovpylib.sparql as sparql\n",
    "import geovpylib.pks as pks\n",
    "import geovpylib.graphs as graphs\n",
    "import geovpylib.find as find\n",
    "\n",
    "db.connect(env, pk_project, execute)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those data correspond to information about persons that already exists on Geovistory. The goal is to enrich them with adding a birth place (sometimes with geo coordinates), and URIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql.init(f\"https://sparql.geovistory.org/api_v1_project_{pk_project}\")\n",
    "\n",
    "data = sparql.query(\"\"\"\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX ontome: <https://ontome.net/ontology/>\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX dbp: <http://dbpedia.org/property/>\n",
    "    PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "\n",
    "\n",
    "    SELECT \n",
    "        ?pk_person ?dbpedia_person_uri ?wikidata_person_uri ?pk_birth ?birthPlace ?birthPlace_type1 ?birthPlace_type2 ?lat ?lng\n",
    "    WHERE {\n",
    "        ?pk_person a ontome:c21 .   \n",
    "        ?pk_person owl:sameAs ?dbpedia_person_uri .\n",
    "        ?pk_birth ontome:p86 ?pk_person\n",
    "        { \n",
    "            SERVICE <https://dbpedia.org/sparql/> { \n",
    "                ?dbpedia_person_uri dbo:birthPlace ?birthPlace .  \n",
    "                optional {?birthPlace dbp:type ?birthPlace_type1 .}\n",
    "                optional {?birthPlace rdf:type ?birthPlace_type2 .}\n",
    "                ?birthPlace geo:lat ?lat .\n",
    "                ?birthPlace geo:long ?lng .\n",
    "      \t\t\t?dbpedia_person_uri owl:sameAs ?wikidata_person_uri .\n",
    "                FILTER (CONTAINS(STR(?birthPlace_type2), 'http://dbpedia.org/ontology/'))\n",
    "    \t\t\tFILTER (CONTAINS(STR(?wikidata_person_uri), 'wikidata'))\n",
    "            } \n",
    "\n",
    "        }\n",
    "    }\n",
    "\"\"\")\n",
    "                    \n",
    "# # For some columns, remove the URI part, just keep the last part\n",
    "data['pk_person'] = [text[text.rindex('/') + 2:] for text in data['pk_person']]\n",
    "data['birthPlace'] = [text[text.rindex('/') + 1:] for text in data['birthPlace']]\n",
    "data['birthPlace_type1'] = [text[text.rindex('/') + 1:] if pd.notna(text) and '/' in text else text for text in data['birthPlace_type1']]\n",
    "data['birthPlace_type2'] = [text[text.rindex('/') + 1:] if pd.notna(text) and '/' in text else text for text in data['birthPlace_type2']]\n",
    "data['pk_birth'] = [text[text.rindex('/') + 2:] if pd.notna(text) and '/' in text else text for text in data['pk_birth']]\n",
    "\n",
    "# Aggregate birth place types\n",
    "birthplaces_t1 = data.groupby(['pk_person', 'birthPlace'])['birthPlace_type1'].apply(lambda x: ', '.join(x.dropna().drop_duplicates())).reset_index()\n",
    "birthplaces_t2 = data.groupby(['pk_person', 'birthPlace'])['birthPlace_type2'].apply(lambda x: ', '.join(x.dropna().drop_duplicates())).reset_index()\n",
    "birthplaces_types = birthplaces_t1.merge(birthplaces_t2, on=['pk_person', 'birthPlace'])\n",
    "birthplaces_types['birthplace_types'] =  birthplaces_types['birthPlace_type1'] + ', ' + birthplaces_types['birthPlace_type2']\n",
    "birthplaces_types['birthplace_types'] = [text[2:] if text.startswith(', ') else text for text in birthplaces_types['birthplace_types']]\n",
    "\n",
    "# Merge the types\n",
    "data = data.merge(birthplaces_types, on=['pk_person', 'birthPlace'])\n",
    "\n",
    "# Select only the column we are interested in\n",
    "data = data[['pk_person', 'dbpedia_person_uri', 'wikidata_person_uri', 'pk_birth', 'birthPlace', 'birthplace_types', 'lat', 'lng']].drop_duplicates()\n",
    "\n",
    "# Drop duplicates to only have one record for each [pk_person, birthPlace]\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "u.infos(data, random=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get information about geographical places"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_places = data[['birthPlace', 'birthplace_types', 'lat', 'lng']]\n",
    "geo_places.columns = ['name', 'types', 'lat', 'lng']\n",
    "\n",
    "# We aggregate all types available for a geo place name\n",
    "geo_places_types = geo_places.groupby('name')['types'].apply(lambda x: ','.join(sorted(list(dict.fromkeys((', '.join(x).split(', '))))))).reset_index()\n",
    "\n",
    "# We can now go back to initial geo_place date, and fill with aggregated types\n",
    "geo_places = geo_places.merge(geo_places_types, on='name').drop(columns=['types_x']).rename(columns={'types_y': 'types'})\n",
    "\n",
    "# We want only one row for each geo place\n",
    "geo_places.drop_duplicates(inplace=True)\n",
    "\n",
    "# We make sure that we only have one row for each place\n",
    "unicity = len(geo_places) == len(geo_places['name'].unique())\n",
    "print(f'Unicity: {unicity}, shape with duplicates: {geo_places.shape}')\n",
    "\n",
    "if not unicity:\n",
    "    gb = geo_places.groupby('name').count().reset_index()\n",
    "    names = gb[gb['types'] != 1]['name'].tolist()\n",
    "    print('Remaining duplicates:')\n",
    "    display(geo_places[geo_places['name'].isin(names)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that filter has been done, we observe that there are duplicates because some geographical places have multiple geo coordinates couples. \n",
    "\n",
    "So we just arbitrarily take one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_places.drop_duplicates(subset=['name'], inplace=True)\n",
    "\n",
    "u.infos(geo_places)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record linkage with existing geographical places inside Geovistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "geo_places.reset_index(inplace=True)\n",
    "geo_places['name'] = geo_places['name'].str.lower()\n",
    "geo_places['name'] = geo_places['name'].str.replace('_', ' ')\n",
    "geo_places['lat'] = geo_places['lat'].astype(float)\n",
    "geo_places['lng'] = geo_places['lng'].astype(float)\n",
    "\n",
    "# Record linkage\n",
    "matches = find.find_geoplaces(geo_places, 'index')\n",
    "\n",
    "# drop created columns\n",
    "geo_places.drop(columns=['index'], inplace=True)\n",
    "\n",
    "matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading through the previous table, we see that the matches are likely to be correct. \n",
    "\n",
    "So we assume they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches[['new_name', 'pk_entity']].drop_duplicates()\n",
    "matches.columns = ['name', 'pk_entity']\n",
    "\n",
    "u.infos(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_places = geo_places.merge(matches, on='name', how='left')\n",
    "geo_places['pk_entity'] = geo_places['pk_entity'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, already existing places in Geovistory can be found because in the table, they have a `pk_entity` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.infos(geo_places)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the information we want to import about Geographical places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBpedia URI\n",
    "geo_places['dbpedia_uri'] = 'http://dbpedia.org/resource/' + geo_places['name']\n",
    "\n",
    "# Type\n",
    "for i, place in geo_places.iterrows():\n",
    "    if 'Country' in place['types']: geo_places.at[i, 'pk_type'] = pks.entities.pk_geo_place_country\n",
    "    elif 'City' in place['types']: geo_places.at[i, 'pk_type'] = pks.entities.pk_geo_place_city\n",
    "    elif 'Village' in place['types']: geo_places.at[i, 'pk_type'] = pks.entities.pk_geo_place_village\n",
    "    elif 'Town' in place['types']: geo_places.at[i, 'pk_type'] = pks.entities.pk_geo_place_town\n",
    "    elif 'Departments_of_France' in place['types']: geo_places.at[i, 'pk_type'] = pd.NA\n",
    "    else: geo_places.at[i, 'pk_type'] = pd.NA\n",
    "geo_places.drop(columns=['types'], inplace=True)\n",
    "\n",
    "\n",
    "geo_places = geo_places[['pk_entity', 'name', 'dbpedia_uri', 'pk_type', 'lat', 'lng']]\n",
    "u.infos(geo_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_create = geo_places[pd.isna(geo_places['pk_entity'])].copy()\n",
    "to_update = geo_places[pd.notna(geo_places['pk_entity'])].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new geographical places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the entity\n",
    "to_create['pk_entity'] = db.resources.create(pks.classes.geoPlace, len(to_create))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add names\n",
    "selection = to_create[pd.notna(to_create['name'])]\n",
    "\n",
    "graphs.add_names(\n",
    "    selection['pk_entity'].tolist(),\n",
    "    selection['name'].tolist(),\n",
    "    pks.languages.english\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add URIs\n",
    "selection = pd.notna(to_create['dbpedia_uri'])\n",
    "\n",
    "graphs.add_uris(\n",
    "    selection['pk_entity'].tolist(),\n",
    "    selection['dbpedia_uri'].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create presences\n",
    "selection = to_create[pd.notna(to_create['lat'])]\n",
    "selection = selection[pd.notna(selection['lng'])]\n",
    "\n",
    "graphs.add_geo_coordinates(\n",
    "    selection['pk_entity'].tolist(),\n",
    "    selection['lat'].astype(float).tolist(),\n",
    "    lngs = selection['lng'].astype(float).tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add types\n",
    "selection = to_create[pd.notna(to_create['pk_type'])]\n",
    "\n",
    "db.statements.create(\n",
    "    selection['pk_entity'].tolist(),\n",
    "    pks.properties.geoPlace_hasIdentifyingGeoPlaceType_geoPlaceType,\n",
    "    selection['pk_type'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data so that it has the created pk_entity\n",
    "to_create.rename(columns={'pk_entity':'pk_entity_created'}, inplace=True)\n",
    "geo_places.merge(to_create[['pk_entity_created', 'name']], on='name')\n",
    "geo_places['pk_entity'] = [row['pk_entity'] if pd.notna(row['pk_entity']) else row['pk_entity_created'] for _, row in geo_places.iterrows()]\n",
    "geo_places.drop(columns=['pk_entity_created'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update existing geographical places (manually?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data to persons (to their births)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "data['birthPlace'] = data['birthPlace'].str.lower()\n",
    "data.drop(columns=['birthplace_types', 'lat', 'lng'], inplace=True)\n",
    "geo_places = geo_places[['name', 'pk_entity']].rename(columns={'pk_entity':'pk_geo_place', 'name':'birthPlace'})\n",
    "\n",
    "data = data.merge(geo_places, on='birthPlace', how='inner').drop(columns=['birthPlace'])\n",
    "u.infos(data, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add DBpedia uris\n",
    "selection = data[['pk_person', 'dbpedia_person_uri']].dropna()\n",
    "\n",
    "graphs.add_uris(\n",
    "    selection['pk_person'].tolist(), \n",
    "    selection['dbpedia_person_uri'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add Wikidata uris\n",
    "selection = data[['pk_person', 'wikidata_person_uri']].dropna()\n",
    "\n",
    "graphs.add_uris(\n",
    "    selection['pk_person'].tolist(), \n",
    "    selection['wikidata_person_uri'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add birth place to births\n",
    "selection = data[['pk_birth', 'pk_birth_place']].dropna()\n",
    "\n",
    "db.statements.create(\n",
    "    selection['pk_birth'].tolist(),\n",
    "    pks.properties.period_tookPlaceOnOrWithin_phyThing,\n",
    "    selection['pk_birth_place']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
