{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geovpylib.database as db\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Connect to Yellow database\n",
    "db.connect_yellow('switzerland_and_beyond')\n",
    "\n",
    "# Fetch corpus\n",
    "persons = db.query(\"select * from hls.person\")\n",
    "person = persons.iloc[5441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Finding words, phrases, names and concepts\n",
    "\n",
    "https://course.spacy.io/en/chapter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the center of this library, there is this `nlp` object which is basically a pipeline (see next chapter to learn more) created by **spaCy**. \n",
    "\n",
    "This object contains eveything needed by the pipeline, like special language rules. It can be used as a function to analyze texts.\n",
    "\n",
    "Processed texts produce a `Doc` object, which structure all information parsed by the pipeline, with no loss of information (ie. it only add information). This `Doc` object is basically a Python sequence (eg. can be iterated over).\n",
    "\n",
    "In the `Doc` object you will find `Token` objects. They represent a word, punctuation, ... Each `Token` posess various attributes (more of that later).\n",
    "\n",
    "You can assemble multiple `Token` together in order to form a `Span`. Which is done by slicing the `Doc` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `doc` in a language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(person.notice)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get tokens out of a `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(person.notice)\n",
    "token = doc[0]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a slice of the doc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(person.notice)\n",
    "a_slice = doc[2:10]\n",
    "print(a_slice.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find dates (births and deaths) in `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(person.notice)\n",
    "lendoc = len(doc)\n",
    "\n",
    "for token in doc:\n",
    "    if token.text == 'Naît' and doc[token.i + 1].text == \"le\" and doc[token.i + 2].like_num:\n",
    "        print('Birth date found:', doc[token.i + 2])\n",
    "    if token.text == 'meurt' and doc[token.i + 1].text == \"le\" and doc[token.i + 2].like_num:\n",
    "        print('Death date found:', doc[token.i + 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, trained pipelines let you analyze context-specific information, eg if a `Span` is person name, a word is a verb, etc.\n",
    "\n",
    "How is that done? Under the hood, **spaCy** has statistical models to make those predictions. Usually, pipelines are used to get part-of-speech (*POS*) tags, syntactic dependencies, named entities, ...\n",
    "\n",
    "Pipelines are trained on large datasets and can be updated to fine-tune predictions.\n",
    "\n",
    "Downloading pretrained pipelines can be done with the command `spacy download` command (see more [here](https://spacy.io/usage/processing-pipelines)), and in code, can be loaded with `spacy.load('')` function (returns an `nlp` object)\n",
    "The pipeline also contains the vocabulary, and various information about it.\n",
    "\n",
    "In **spaCy**, attributes suffixed with \"_\" return string values, without underscore, it will only return an integer ID value.\n",
    "\n",
    "Some other exemple of what can be retrieved by a trained pipeline (apart from POS tags) are: dependency (`.dep_` like subjet, object, ...), syntactic head token (`.head`, parent token), named entities (`.ents`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "text = person.notice\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict language annotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "text = person.notice\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc[0:15]:\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    \n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All kinds of POS found, with explaination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "text = person.notice\n",
    "doc = nlp(text)\n",
    "\n",
    "POSs = []\n",
    "DEPs = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in POSs: POSs.append(token.pos_)\n",
    "    if token.dep_ not in DEPs: DEPs.append(token.dep_)\n",
    "\n",
    "print('===== Part of Speech: =====')\n",
    "for pos in POSs:\n",
    "    print(pos, \"-->\", spacy.explain(pos))\n",
    "\n",
    "print('\\n===== Dependency labels: =====')\n",
    "for dep in DEPs:\n",
    "    print(dep, \"==>\", spacy.explain(dep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All entities found in a text (NER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "text = person.notice\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"==>\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find matchings in texts, **spaCy** matchings works as regular expression in `Doc` and `Token`. We can find texts, lexical attributes, etc.\n",
    "\n",
    "Patterns used to find matchings are lists of dictonaries representing token attributes (lower case version of strings, optional tokens, forms of spans, punctuations, ...). Matchings will be a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "text = person.notice\n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "pattern_birth = [{'TEXT': 'Naît'}, {'TEXT': 'le'}, {'LIKE_NUM': True}]\n",
    "pattern_death = [{'TEXT': 'meurt'}, {'TEXT': 'le'}, {'LIKE_NUM': True}]\n",
    "pattern_son = [{'TEXT': 'Fils'}, {'TEXT': 'de'}, {'POS': 'PROPN'}]\n",
    "pattern_daughter = [{'TEXT': 'Fille'}, {'TEXT': 'de'}, {'POS': 'PROPN'}]\n",
    "\n",
    "matcher.add(\"BIRTH\", [pattern_birth])\n",
    "matcher.add(\"DEATH\", [pattern_death])\n",
    "matcher.add(\"SON\", [pattern_son])\n",
    "matcher.add(\"DAUGHTER\", [pattern_daughter])\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "for id, start, end in matches:\n",
    "    print(doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** stores all shared data (is the word alphabetic, the text itself, ...) in a vocabulary. Internally, to increase performance and memory, it only uses hashed versions of words. Vocabulary can be extended manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word hashes (in vocab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "text = person.notice\n",
    "doc = nlp(text)\n",
    "word = 'meurt'\n",
    "hash = nlp.vocab.strings[word]\n",
    "word_from_hash = nlp.vocab.strings[hash]\n",
    "\n",
    "print(hash, word_from_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central data structure is the `Doc` object, created by calling the `nlp` function on a text. But `Doc` can also be created manually.\n",
    "\n",
    "A `Span` can also be manually created by calling it on a `Doc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manually create a `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a new entity to the existing entities of a `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('fr_core_news_sm')\n",
    "nlp = spacy.blank('fr')\n",
    "text = person.notice\n",
    "doc = nlp(text)\n",
    "\n",
    "span = spacy.tokens.Span(doc, 35, 36, label=\"PERSON\")\n",
    "doc.ents = [span]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All proper nouns followed by a verb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(person.notice)\n",
    "\n",
    "for token in doc:\n",
    "    # Is current word a proper noun?\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Is next word a verb?\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"- \", token.text, doc[token.i + 1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** is capable of comparing word similarity through vector representation of them.\n",
    "\n",
    "To use this similarity function, pipelines need to have it in them (small pipelines do not have it), find more about them [here](https://spacy.io/models).\n",
    "\n",
    "Similarity scores express how much 2 words are similar, range from 0 (totally different) to 1 (same meaning).\n",
    "\n",
    "By default similarity scores come from a [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between the 2 vectors representing the 2 words. \n",
    "\n",
    "In order to \"transform\" words into vectors, **spaCy** uses [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), which does the embedding (process of transforming texts to numbers)\n",
    "\n",
    "To have a vector from multiple tokens (like a `Doc` or a `Span`), it is the average of all token vectors that is sent back. That is why the embedding has more value with fewer irrelevant words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(person.notice)\n",
    "\n",
    "protestant_vector = doc[17].vector\n",
    "print(protestant_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "# Compare 2 documents\n",
    "doc1 = nlp(person.notice.split(', ')[0])\n",
    "doc2 = nlp(person.notice.split(', ')[1])\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Compare 2 tokens\n",
    "doc = nlp(person.notice)\n",
    "token1 = doc[0] # Naît\n",
    "token2 = doc[11] # Meurt\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "# Compare 2 spans\n",
    "doc = nlp(person.notice)\n",
    "span1 = doc[29:41] # directeur de l'école rurale de la Pommière à Chêne-Bougeries\n",
    "span2 = doc[53:59] # Instituteur à l'école de Plainpalais\n",
    "print(span1.similarity(span2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining predictions and rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining statistical prediction and rule based system is the most powerfull trick one can have in his NLP toolbox.\n",
    "\n",
    "Statistical predictions are powerfull to predict if a span of tokens are person names for exemple, or another exemple is to find relationships between subject and objects.\n",
    "On the other hand, rule-based approaches are handy if there is a finite numbers of instances you want to find (country names, cities, ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find matchings in texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(person.notice)\n",
    "\n",
    "# Define Patterns\n",
    "pattern_cons_munic = [{'LOWER':'conseil'}, {'LOWER': 'municipal'}]\n",
    "pattern_cons_natio = [{'LEMMA':'conseiller'}, {'LOWER': 'national'}]\n",
    "\n",
    "# Add the Patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('CONSEIL_MUNIC', [pattern_cons_munic])\n",
    "matcher.add('CONSEIL_NATIO', [pattern_cons_natio])\n",
    "\n",
    "# Find matchings\n",
    "matchings = matcher(doc)\n",
    "\n",
    "# Inspect matchings\n",
    "for id, start, end in matchings:\n",
    "    print(doc.vocab.strings[id], doc[start:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Match exact strings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more efficient than the other techniques, but can have lower metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(person.notice)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(['Conseil Municipal', 'Conseil municipal', 'conseiller municipal', 'Conseil National', 'conseiller national']))\n",
    "# patterns = [nlp(role) for role in LIST]\n",
    "matcher.add('POLITICIAN', patterns)\n",
    "\n",
    "matchings = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matchings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get relationship between given entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(person.notice)\n",
    "doc.ents = [] # Reset the ones created by the pipeline\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(['Conseil Municipal', 'Conseil municipal', 'conseiller municipal', 'Conseil National', 'conseiller national']))\n",
    "matcher.add('POLITICIAN', patterns)\n",
    "matchings = matcher(doc)\n",
    "\n",
    "# Add the matches to the entities\n",
    "for id, start, end in matchings:\n",
    "    span = Span(doc, start, end, label=\"POLITIC_ROLE\")\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    span_root_head = span.root.head\n",
    "    print(span_root_head, '-->', span.text)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"POLITIC_ROLE\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
