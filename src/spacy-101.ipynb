{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "notice = \"\"\"\n",
    "Naît le 31.5.1877 à Choindez (commune de Courrendlin), meurt le 31.5.1931 à Genève, protestant, de Genève en 1902.\n",
    "Fils d'Henri Albert, directeur de l'école rurale de la Pommière à Chêne-Bougeries, et d'Anna Grosvernier.\n",
    "Célibataire. Etudes au collège. Instituteur à l'école de Plainpalais (aujourd'hui commune de Genève) en 1908,\n",
    "puis à celles de Vernier, des Pâquis (Genève), de Versoix, de Carl-Vogt, de La Roseraie et du Grütli (les trois dernières à Genève).\n",
    "Membre socialiste du Conseil municipal (législatif) de Genève (1914-1931, président en 1922) et conseiller national (1922).\n",
    "Ernest Joray a également présidé le comité de l'université ouvrière de Genève, de 1910 à sa mort.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** code snippets and explaination from https://course.spacy.io/en/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the center of this library, there is this `nlp` object which is basically a pipeline (see next chapter to learn more) created by **spaCy**. \n",
    "\n",
    "This object contains eveything needed by the pipeline, like special language rules. It can be used as a function to analyze texts.\n",
    "\n",
    "Processed texts produce a `Doc` object, which structure all information parsed by the pipeline, with no loss of information (ie. it only add information). This `Doc` object is basically a Python sequence (eg. can be iterated over).\n",
    "\n",
    "In the `Doc` object you will find `Token` objects. They represent a word, punctuation, ... Each `Token` posess various attributes (more of that later).\n",
    "\n",
    "You can assemble multiple `Token` together in order to form a `Span`. Which is done by slicing the `Doc` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `doc` in a language**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(notice)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get tokens out of a `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(notice)\n",
    "token = doc[0]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a slice of the doc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(notice)\n",
    "a_slice = doc[2:10]\n",
    "print(a_slice.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find dates (births and deaths) in `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(notice)\n",
    "lendoc = len(doc)\n",
    "\n",
    "for token in doc:\n",
    "    if token.text == 'Naît' and doc[token.i + 1].text == \"le\" and doc[token.i + 2].like_num:\n",
    "        print('Birth date found:', doc[token.i + 2])\n",
    "    if token.text == 'meurt' and doc[token.i + 1].text == \"le\" and doc[token.i + 2].like_num:\n",
    "        print('Death date found:', doc[token.i + 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, trained pipelines let you analyze context-specific information, eg if a `Span` is person name, a word is a verb, etc.\n",
    "\n",
    "How is that done? Under the hood, **spaCy** has statistical models to make those predictions. Usually, pipelines are used to get part-of-speech (*POS*) tags, syntactic dependencies, named entities, ...\n",
    "\n",
    "Pipelines are trained on large datasets and can be updated to fine-tune predictions.\n",
    "\n",
    "Downloading pretrained pipelines can be done with the command `spacy download` command (see more [here](https://spacy.io/usage/processing-pipelines)), and in code, can be loaded with `spacy.load('')` function (returns an `nlp` object)\n",
    "The pipeline also contains the vocabulary, and various information about it.\n",
    "\n",
    "In **spaCy**, attributes suffixed with \"_\" return string values, without underscore, it will only return an integer ID value.\n",
    "\n",
    "Some other exemple of what can be retrieved by a trained pipeline (apart from POS tags) are: dependency (`.dep_` like subjet, object, ...), syntactic head token (`.head`, parent token), named entities (`.ents`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(notice)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict language annotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(notice)\n",
    "\n",
    "for token in doc[0:15]:\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    \n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All kinds of POS found, with explaination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(notice)\n",
    "\n",
    "POSs = []\n",
    "DEPs = []\n",
    "for token in doc:\n",
    "    if token.pos_ not in POSs: POSs.append(token.pos_)\n",
    "    if token.dep_ not in DEPs: DEPs.append(token.dep_)\n",
    "\n",
    "print('===== Part of Speech: =====')\n",
    "for pos in POSs:\n",
    "    print(pos, \"-->\", spacy.explain(pos))\n",
    "\n",
    "print('\\n===== Dependency labels: =====')\n",
    "for dep in DEPs:\n",
    "    print(dep, \"==>\", spacy.explain(dep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All entities found in a text (NER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(notice)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"==>\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find matchings in texts, **spaCy** matchings works as regular expression in `Doc` and `Token`. We can find texts, lexical attributes, etc.\n",
    "\n",
    "Patterns used to find matchings are lists of dictonaries representing token attributes (lower case version of strings, optional tokens, forms of spans, punctuations, ...). Matchings will be a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(notice)\n",
    "\n",
    "matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "pattern_birth = [{'TEXT': 'Naît'}, {'TEXT': 'le'}, {'LIKE_NUM': True}]\n",
    "pattern_death = [{'TEXT': 'meurt'}, {'TEXT': 'le'}, {'LIKE_NUM': True}]\n",
    "pattern_son = [{'TEXT': 'Fils'}, {'TEXT': 'de'}, {'POS': 'PROPN'}]\n",
    "pattern_daughter = [{'TEXT': 'Fille'}, {'TEXT': 'de'}, {'POS': 'PROPN'}]\n",
    "\n",
    "matcher.add(\"BIRTH\", [pattern_birth])\n",
    "matcher.add(\"DEATH\", [pattern_death])\n",
    "matcher.add(\"SON\", [pattern_son])\n",
    "matcher.add(\"DAUGHTER\", [pattern_daughter])\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "for id, start, end in matches:\n",
    "    print(doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** stores all shared data (is the word alphabetic, the text itself, ...) in a vocabulary. Internally, to increase performance and memory, it only uses hashed versions of words. Vocabulary can be extended manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word hashes (in vocab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(notice)\n",
    "word = 'meurt'\n",
    "hash = nlp.vocab.strings[word]\n",
    "word_from_hash = nlp.vocab.strings[hash]\n",
    "\n",
    "print(hash, word_from_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central data structure is the `Doc` object, created by calling the `nlp` function on a text. But `Doc` can also be created manually.\n",
    "\n",
    "A `Span` can also be manually created by calling it on a `Doc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manually create a `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a new entity to the existing entities of a `doc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('fr_core_news_sm')\n",
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(notice)\n",
    "\n",
    "span = spacy.tokens.Span(doc, 35, 36, label=\"PERSON\")\n",
    "doc.ents = [span]\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All proper nouns followed by a verb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(notice)\n",
    "\n",
    "for token in doc:\n",
    "    # Is current word a proper noun?\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Is next word a verb?\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"- \", token.text, doc[token.i + 1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** is capable of comparing word similarity through vector representation of them.\n",
    "\n",
    "To use this similarity function, pipelines need to have it in them (small pipelines do not have it), find more about them [here](https://spacy.io/models).\n",
    "\n",
    "Similarity scores express how much 2 words are similar, range from 0 (totally different) to 1 (same meaning).\n",
    "\n",
    "By default similarity scores come from a [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between the 2 vectors representing the 2 words. \n",
    "\n",
    "In order to \"transform\" words into vectors, **spaCy** uses [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), which does the embedding (process of transforming texts to numbers)\n",
    "\n",
    "To have a vector from multiple tokens (like a `Doc` or a `Span`), it is the average of all token vectors that is sent back. That is why the embedding has more value with fewer irrelevant words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(notice)\n",
    "\n",
    "protestant_vector = doc[17].vector\n",
    "print(protestant_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "# Compare 2 documents\n",
    "doc1 = nlp(notice.split(', ')[0])\n",
    "doc2 = nlp(notice.split(', ')[1])\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "# Compare 2 tokens\n",
    "doc = nlp(notice)\n",
    "token1 = doc[0] # Naît\n",
    "token2 = doc[11] # Meurt\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "# Compare 2 spans\n",
    "doc = nlp(notice)\n",
    "span1 = doc[29:41] # directeur de l'école rurale de la Pommière à Chêne-Bougeries\n",
    "span2 = doc[53:59] # Instituteur à l'école de Plainpalais\n",
    "print(span1.similarity(span2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining predictions and rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining statistical prediction and rule based system is the most powerfull trick one can have in his NLP toolbox.\n",
    "\n",
    "Statistical predictions are powerfull to predict if a span of tokens are person names for exemple, or another exemple is to find relationships between subject and objects.\n",
    "On the other hand, rule-based approaches are handy if there is a finite numbers of instances you want to find (country names, cities, ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find matchings in texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "doc = nlp(notice)\n",
    "\n",
    "# Define Patterns\n",
    "pattern_cons_munic = [{'LOWER':'conseil'}, {'LOWER': 'municipal'}]\n",
    "pattern_cons_natio = [{'LEMMA':'conseiller'}, {'LOWER': 'national'}]\n",
    "\n",
    "# Add the Patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('CONSEIL_MUNIC', [pattern_cons_munic])\n",
    "matcher.add('CONSEIL_NATIO', [pattern_cons_natio])\n",
    "\n",
    "# Find matchings\n",
    "matchings = matcher(doc)\n",
    "\n",
    "# Inspect matchings\n",
    "for id, start, end in matchings:\n",
    "    print(doc.vocab.strings[id], doc[start:end])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Match exact strings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more efficient than the other techniques, but can have lower metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.blank('fr')\n",
    "doc = nlp(notice)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(['Conseil Municipal', 'Conseil municipal', 'conseiller municipal', 'Conseil National', 'conseiller national']))\n",
    "# patterns = [nlp(role) for role in LIST]\n",
    "matcher.add('POLITICIAN', patterns)\n",
    "\n",
    "matchings = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matchings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get relationship between given entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(notice)\n",
    "doc.ents = [] # Reset the ones created by the pipeline\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(['Conseil Municipal', 'Conseil municipal', 'conseiller municipal', 'Conseil National', 'conseiller national']))\n",
    "matcher.add('POLITICIAN', patterns)\n",
    "matchings = matcher(doc)\n",
    "\n",
    "# Add the matches to the entities\n",
    "for id, start, end in matchings:\n",
    "    span = Span(doc, start, end, label=\"POLITIC_ROLE\")\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    span_root_head = span.root.head\n",
    "    print(span_root_head, '-->', span.text)\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"POLITIC_ROLE\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be done in a **spaCy** pipeline, and what happens behind the scene.\n",
    "\n",
    "When applying a pipeline to a string, it first applies the tokenizer, followed by a series of component (pipeline component, can be parser, entity recognizer, POS tagger, ...) and returns a `Doc` object for the developer to work with.\n",
    "\n",
    "Built in pipeline component:\n",
    "- Part-of-speech tagger: sets the `token.tag` and the `token.pos` attribute\n",
    "- Dependency parser: sets the `token.dep` and `token.head` attributes; but also the sentences and base noun phrases (noun chunks)\n",
    "- Named entity recognizer: sets the `doc.ents`; and sets atrtibutes to know if a token is part of an entity or not.\n",
    "- Text classifier: sets the `doc.cats` property (category label that apply to the whole text)\n",
    "\n",
    "Beware! Text classifier is not by default in any pretrained pipeline, because it is always very usecase specific. It can be used to train a new system.\n",
    "\n",
    "\n",
    "A pipeline is made of several folders, binary files, and a configuration `config.cfg`. It defines languages and pipeline's component, how they should be configured, applyed, etc.\n",
    "\n",
    "The list of a pipeline's components can be accessed via `nlp.pipe_names` or `nlp.pipeline` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List pipeline components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "print('Pipeline names')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "print('Pipeline labels')\n",
    "print(nlp.pipe_labels)\n",
    "\n",
    "print('Pipeline components')\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is usefull for example to add custom pipelines components for special needs or also to update built-in attributes, like named entity spans.\n",
    "\n",
    "In the end, a pipeline component is a callable function taking a `Doc`, update it, and returns it.\n",
    "\n",
    "To create such a function (pipeline component), add the `spacy.Language.component(\"component_name\")` decorator to a new function. After that, the function can be simply added with `nlp.add_pipe(\"component_name\")`. When adding the new component name one can set its place using the `last`, `first`, `before`, `after` keywords. By default it is append in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new custom pipeline component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spacy.Language.component('length_component')\n",
    "def length_component(doc):\n",
    "    doc_length = len(doc)\n",
    "    print(f'This document is {doc_length} tokens long.')\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "nlp.add_pipe('length_component', first=True)\n",
    "doc = nlp(notice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More complex component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "parents = ['Henri Albert', 'Anna Grosvernier']\n",
    "parents_patterns = list(nlp.pipe(parents))\n",
    "matcher = spacy.matcher.PhraseMatcher(nlp.vocab)\n",
    "matcher.add('PARENT', parents_patterns)\n",
    "\n",
    "@spacy.Language.component('parent_component')\n",
    "def parent_component(doc):\n",
    "    matchings = matcher(doc)\n",
    "    spans = [spacy.tokens.Span(doc, start, end, label=\"PARENT\") for id, start, end in matchings]\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe('parent_component', after='ner')\n",
    "doc = nlp(notice)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add custom attributes to either `token`, `Doc` or `Span`, they have to be added to the `_` attribute.\n",
    "\n",
    "Another solution is to add them directly to the global class with the `set_extension` function (they will also be available in the `_` attribute).\n",
    "\n",
    "Extensions can be attribute (variable), property (variable with getter and setter) or method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add an attribute extension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "spacy.tokens.Token.set_extension(\"is_town\", default=False)\n",
    "\n",
    "doc = nlp(notice)\n",
    "doc[5]._.is_town = True\n",
    "print([(token.text, token._.is_town) for token in doc[0:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a property extension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# spacy.tokens.Token.set_extension('reversed', getter=get_reversed)\n",
    "\n",
    "doc = nlp(notice)\n",
    "token = doc[5]\n",
    "print('Token   :', token.text)\n",
    "print('Reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add a method extension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "\n",
    "def to_html(span, tag):\n",
    "    return f'<{tag}>{span.text}</{tag}>'\n",
    "\n",
    "spacy.tokens.Span.set_extension('to_html', method=to_html)\n",
    "\n",
    "doc = nlp(notice)\n",
    "span = doc[4:6]\n",
    "print(span._.to_html('i'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a HLS link for all persons mentioned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "def get_hls_findings(span):\n",
    "    if span.label_ == 'PER':\n",
    "        text = span.text.replace(' ', '%20')\n",
    "        return f'https://hls-dhs-dss.ch/fr/search/?text={text}'\n",
    "\n",
    "spacy.tokens.Span.set_extension('hls_url', getter=get_hls_findings)\n",
    "\n",
    "\n",
    "doc = nlp(notice)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent._.hls_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way of creating a lot of `Doc`, is to use the `pipe` method:\n",
    "\n",
    "```python\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXTS] # BAD\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXTS)) # GOOD\n",
    "```\n",
    "\n",
    "In order to pass additional information about docs, metadata can be added, and docs can be passed as tuples (`as_tuples` option has to be `True`).\n",
    "\n",
    "Sometimes, one can only want to have a `Doc` from a text. Calling the full pipeline could be useless and CPU consuming, so to only tokenize the text, better just calling `nlp.make_doc`.\n",
    "\n",
    "Likewise, it is also possible to enable/disable pipeline components, in order to have specific uses: \n",
    "\n",
    "```python\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\"]):\n",
    "    doc = nlp(text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and updating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update an existing model, data needed is from a few hundred, to thousands.\n",
    "To train a new category, it may be needed to have more than a million training data.\n",
    "\n",
    "To illustrate, the **spaCy** english model was trained on more than 2 millions words labelled with POS tags, dependencies, and named entities.\n",
    "\n",
    "Classically in AI, a testing (evaluation) dataset is also needed, to check how the model learns.\n",
    "\n",
    "Training and evaluation dataset needs to be docs as they should be created by the model (`Doc`s objects with `ents`, `pos`, ... attributes).\n",
    "\n",
    "Of course, to increase performance, datasets (training and evaluation) can be stored as binary files, for that, the `DocBin` object can be used (`.spacy` extension used for those files). It is more efficient, and creates smaller files than the pickle format.\n",
    "More of that [here](https://spacy.io/api/docbin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create training/testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Henri Albert, Anna Grosvernier]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span, DocBin\n",
    "\n",
    "nlp = spacy.blank('fr')\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1 = [{'LOWER': 'henri'}, {'LOWER': 'albert'}]\n",
    "pattern2 = [{'LOWER': 'anna'}, {'LOWER': 'grosvernier'}]\n",
    "matcher.add('PARENT', [pattern1, pattern2])\n",
    "docs = []\n",
    "for doc in nlp.pipe([notice]): # Here we simulate that we have multiple notices\n",
    "    matchings = matcher(doc)\n",
    "    spans = [Span(doc, start, end) for id, start, end in matchings]\n",
    "    print(spans)\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)\n",
    "\n",
    "doc_bin = DocBin(docs=docs)\n",
    "doc_bin.to_disk('./train.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the section before, the training configuration **has to** be set in the configuration file (`config.cfg`)\n",
    "\n",
    "This configuration files is the *single source of truth* for all **spaCy** settings, going from how the `nlp` object is created, the list of components and their internal model configuration, to all the training parameters like how to load data, training hyperparameters, ...\n",
    "\n",
    "But configuration files does required to be created by hand, **spaCy** can do that automatically (see [here](https://spacy.io/usage/training#quickstart) and [here](https://spacy.io/api/cli#init-config) for more info).\n",
    "\n",
    "Once a pipeline is trained, it is loadable as a normal **spaCy** pipeline with the `spacy.load(pipeline_name)`.\n",
    "\n",
    "One can also packages his pipeline with the [spacy package command](https://spacy.io/api/cli#package), which ease the deployment and the loading process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a configuration file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Generate a config file in french, with only one component: Named Entity Recognition\n",
    "python -m spacy init config ./config.cfg --lang fr --pipeline ner\n",
    "\n",
    "# Inspect the generated config\n",
    "cat ./config.cfg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train using the CLI**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m spacy train ./config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./test.spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is normal that training does not go as one wants the first time he tries it. It is an iterative process where things need to be tested.\n",
    "\n",
    "- In the training data, data about already correct prediction is better to be included, to avoid the forgetting problem. So in the end, training data would then mix those data with one's own new data.\n",
    "- The model will have trouble learning things from context: it will be difficult to learn to distinguish adult clothing from children clothing for example. It is better to have generic objectives.\n",
    "\n",
    "\n",
    "To create training data (i.e. make annotations) tools should be used: [Brat](http://brat.nlplab.org/), (open source solution), or [Prodigy](https://prodi.gy/) (integrates with **spaCy**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has been touched in this notebook:\n",
    "- Extract linguistic features: part-of-speech tags, dependencies, named entities\n",
    "- Work with trained pipelines\n",
    "- Find words and phrases using Matcher and PhraseMatcher match rules\n",
    "- Best practices for working with data structures Doc, Token Span, Vocab, Lexeme\n",
    "- Find semantic similarities using word vectors\n",
    "- Write custom pipeline components with extension attributes\n",
    "- Scale up your spaCy pipelines and make them fast\n",
    "- Create training data for spaCy's statistical models\n",
    "- Train and update spaCy's neural network models with new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
