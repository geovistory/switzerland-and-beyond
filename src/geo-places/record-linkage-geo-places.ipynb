{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = 'staging'\n",
    "pk_project = 153\n",
    "execute = False\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "\n",
    "import geovpylib.analysis as a\n",
    "import geovpylib.database as db\n",
    "import geovpylib.graphs as graphs\n",
    "import geovpylib.pks as pks\n",
    "import geovpylib.recordlinkage as rl\n",
    "import geovpylib.sparql as sparql\n",
    "import geovpylib.utils as u\n",
    "\n",
    "eta = u.Eta()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch all geographical places"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing persons birth & death places"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the person already in the project, we fetch the information in dbpedia in order to have birth places and death places. \n",
    "\n",
    "We can have thoes information, because in Geovistory, there already is the dbpedia URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> SPARQL endpoint of project 153 set.\n"
     ]
    }
   ],
   "source": [
    "sparql.connect_geovistory(pk_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'types'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'types'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-984ebd17cc80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mdata_existing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_existing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://dbpedia.org/ontology/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_existing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'place'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'types'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mdata_existing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'types'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_existing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'types'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mdata_existing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_existing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'place'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mdata_existing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'types'"
     ]
    }
   ],
   "source": [
    "data_existing = pd.concat([\n",
    "    sparql.query(\"\"\"\n",
    "        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "        PREFIX ontome: <https://ontome.net/ontology/>\n",
    "        PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "        PREFIX dbp: <http://dbpedia.org/property/>\n",
    "        PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "\n",
    "        SELECT DISTINCT\n",
    "            ?place ?type ?lat ?lng\n",
    "        WHERE {\n",
    "            ?pk_person a ontome:c21 .   \n",
    "            ?pk_person owl:sameAs ?dbpedia_person_uri .\n",
    "            { \n",
    "                SERVICE <https://dbpedia.org/sparql/> { \n",
    "                    ?dbpedia_person_uri dbo:birthPlace ?place .  \n",
    "                    optional {?place rdf:type ?type .}\n",
    "                    ?place geo:lat ?lat .\n",
    "                    ?place geo:long ?lng .\n",
    "                    FILTER (CONTAINS(STR(?type2), 'http://dbpedia.org/ontology/'))\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    \"\"\"), \n",
    "    sparql.query(\"\"\"\n",
    "        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "        PREFIX ontome: <https://ontome.net/ontology/>\n",
    "        PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "        PREFIX dbp: <http://dbpedia.org/property/>\n",
    "        PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "\n",
    "        SELECT DISTINCT\n",
    "            ?place ?type ?lat ?lng\n",
    "        WHERE {\n",
    "            ?pk_person a ontome:c21 .   \n",
    "            ?pk_person owl:sameAs ?dbpedia_person_uri .\n",
    "            { \n",
    "                SERVICE <https://dbpedia.org/sparql/> { \n",
    "                    ?dbpedia_person_uri dbo:deathPlace ?place .  \n",
    "                    optional {?place rdf:type ?type .}\n",
    "                    ?place geo:lat ?lat .\n",
    "                    ?place geo:long ?lng .\n",
    "                    FILTER (CONTAINS(STR(?type), 'http://dbpedia.org/ontology/'))\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "# Because we might have duplicated places that have been birth places and death places \n",
    "data_existing.drop_duplicates(inplace=True)\n",
    "\n",
    "# Handle types\n",
    "data_existing['type'] = data_existing['type'].str.replace('http://dbpedia.org/ontology/', '', regex=False)\n",
    "types = data_existing.groupby('place')['type'].apply(lambda x: ', '.join(x.dropna().drop_duplicates())).reset_index().rename(columns={'type': 'types'})\n",
    "data_existing['types'] = data_existing['types'].str.lower()\n",
    "data_existing = data_existing.merge(types, on='place').drop(columns=['type'])\n",
    "data_existing.drop_duplicates(inplace=True)\n",
    "\n",
    "# Get the name\n",
    "data_existing['name'] = data_existing['place'].str.replace('http://dbpedia.org/resource/', '', regex=False).str.replace('_', ' ')\n",
    "\n",
    "# Rename columns\n",
    "data_existing.rename(columns={'place':'uri'}, inplace=True)\n",
    "\n",
    "# Column reordering \n",
    "data_existing = data_existing[['name', 'lat', 'lng', 'types', 'uri']]\n",
    "\n",
    "a.infos(data_existing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HLS birth and death places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparql.connect_external('https://query.wikidata.org/sparql')\n",
    "\n",
    "birthplaces = sparql.query(\"\"\"\n",
    "    SELECT ?place ?placeLabel ?typeLabel ?lat ?lng\n",
    "    WHERE {\n",
    "        ?uri_wikidata wdt:P902 ?uri_hls .\n",
    "        ?uri_wikidata wdt:P31 wd:Q5 .\n",
    "        ?uri_wikidata wdt:P19 ?place .\n",
    "        ?place wdt:P31 ?type .\n",
    "        ?place p:P625 ?coords .\n",
    "        ?coords psv:P625 ?coordinate_node.\n",
    "        ?coordinate_node wikibase:geoLatitude ?lat .\n",
    "        ?coordinate_node wikibase:geoLongitude ?lng .\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en,fr,it\". }\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "deathplaces = sparql.query(\"\"\"\n",
    "    SELECT ?place ?placeLabel ?typeLabel ?lat ?lng\n",
    "    WHERE {\n",
    "        ?uri_wikidata wdt:P902 ?uri_hls .\n",
    "        ?uri_wikidata wdt:P31 wd:Q5 .\n",
    "        ?uri_wikidata wdt:P20 ?place .\n",
    "        ?place wdt:P31 ?type .\n",
    "        ?place p:P625 ?coords .\n",
    "        ?coords psv:P625 ?coordinate_node.\n",
    "        ?coordinate_node wikibase:geoLatitude ?lat .\n",
    "        ?coordinate_node wikibase:geoLongitude ?lng .\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en,fr,it\". }\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "# Merge birth places and deah places                           \n",
    "data_hls = pd.concat([birthplaces, deathplaces])\n",
    "data_hls.drop_duplicates(inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "data_hls.columns = ['uri', 'lat', 'lng', 'name', 'type']\n",
    "\n",
    "# Aggregate types\n",
    "data_hls['types'] = data_hls.groupby('uri')['type'].transform(lambda x: ', '.join(x))\n",
    "data_hls['types'] = data_hls['types'].str.lower()\n",
    "data_hls.drop(columns=['type'], inplace=True)\n",
    "data_hls.drop_duplicates(subset=['uri'], inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "data_hls = data_hls[['name', 'uri', 'lat', 'lng', 'types']]\n",
    "data_hls = u.parse_df(data_hls)\n",
    "\n",
    "a.infos(data_hls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge both dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_existing, data_hls])\n",
    "\n",
    "a.infos(data, random=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type analysis\n",
    "types_analysis = []\n",
    "\n",
    "all_types = np.unique(', '.join(data['types'].tolist()).split(', '))\n",
    "total_nb = len(data)\n",
    "for type in all_types:\n",
    "    nb = data['types'].str.contains(type).sum()\n",
    "    types_analysis.append({'type': type, 'nb': nb, 'percent': u.percent(nb / total_nb)})\n",
    "\n",
    "types_analysis = pd.DataFrame(data=types_analysis).sort_values('nb')\n",
    "types_analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis:\n",
    "When those words are present in the types, we take the mentioned geo place type. We also take the priority order as it is displayed:\n",
    "- When 'Village' then `pk_geo_place_type = 732859`\n",
    "- When 'Town' then `pk_geo_place_type = 80412`\n",
    "- When 'City' then `pk_geo_place_type = 80426`\n",
    "- When 'Country' then `pk_geo_place_type = 919118`\n",
    "- When 'Region' then `pk_geo_place_type = 3236783`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(types):\n",
    "    if 'Village' in types: return 'Village'\n",
    "    if 'Town' in types: return 'Town'\n",
    "    if 'City' in types: return 'City'\n",
    "    if 'Country' in types: return 'Country'\n",
    "    if 'Region' in types: return 'Region'\n",
    "    return pd.NA\n",
    "\n",
    "def get_pk_type(types):\n",
    "    if 'Village' in types: return 732859\n",
    "    if 'Town' in types: return 80412\n",
    "    if 'City' in types: return 80426\n",
    "    if 'Country' in types: return 919118\n",
    "    if 'Region' in types: return 3236783\n",
    "\n",
    "\n",
    "data['type'] = [get_type(types) for types in data['types']]\n",
    "data['pk_type'] = [get_pk_type(types) for types in data['types']]\n",
    "data['pk_type'] = data['pk_type'].astype(pd.Int64Dtype())\n",
    "\n",
    "a.infos(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still Geo place that could have been parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[pd.isna(data['type'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sparql.query(\"\"\"\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX ontome: <https://ontome.net/ontology/>\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX dbp: <http://dbpedia.org/property/>\n",
    "    PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "\n",
    "    SELECT \n",
    "        ?pk_person ?dbpedia_person_uri ?wikidata_person_uri ?pk_birth ?birthPlace ?birthPlace_type1 ?birthPlace_type2 ?lat ?lng\n",
    "    WHERE {\n",
    "        ?pk_person a ontome:c21 .   \n",
    "        ?pk_person owl:sameAs ?dbpedia_person_uri .\n",
    "        ?pk_birth ontome:p86 ?pk_person\n",
    "        { \n",
    "            SERVICE <https://dbpedia.org/sparql/> { \n",
    "                ?dbpedia_person_uri dbo:birthPlace ?birthPlace .  \n",
    "                optional {?birthPlace dbp:type ?birthPlace_type1 .}\n",
    "                optional {?birthPlace rdf:type ?birthPlace_type2 .}\n",
    "                ?birthPlace geo:lat ?lat .\n",
    "                ?birthPlace geo:long ?lng .\n",
    "      \t\t\t?dbpedia_person_uri owl:sameAs ?wikidata_person_uri .\n",
    "                FILTER (CONTAINS(STR(?birthPlace_type2), 'http://dbpedia.org/ontology/'))\n",
    "    \t\t\tFILTER (CONTAINS(STR(?wikidata_person_uri), 'wikidata'))\n",
    "            } \n",
    "\n",
    "        }\n",
    "    }\n",
    "\"\"\")\n",
    "                    \n",
    "# For some columns, remove the URI part, just keep the last part\n",
    "data['pk_person'] = [text[text.rindex('/') + 2:] for text in data['pk_person']]\n",
    "data['birthPlace'] = [text[text.rindex('/') + 1:] for text in data['birthPlace']]\n",
    "data['birthPlace_type1'] = [text[text.rindex('/') + 1:] if pd.notna(text) and '/' in text else text for text in data['birthPlace_type1']]\n",
    "data['birthPlace_type2'] = [text[text.rindex('/') + 1:] if pd.notna(text) and '/' in text else text for text in data['birthPlace_type2']]\n",
    "data['pk_birth'] = [text[text.rindex('/') + 2:] if pd.notna(text) and '/' in text else text for text in data['pk_birth']]\n",
    "\n",
    "# Aggregate birth place types\n",
    "birthplaces_t1 = data.groupby(['pk_person', 'birthPlace'])['birthPlace_type1'].apply(lambda x: ', '.join(x.dropna().drop_duplicates())).reset_index()\n",
    "birthplaces_t2 = data.groupby(['pk_person', 'birthPlace'])['birthPlace_type2'].apply(lambda x: ', '.join(x.dropna().drop_duplicates())).reset_index()\n",
    "birthplaces_types = birthplaces_t1.merge(birthplaces_t2, on=['pk_person', 'birthPlace'])\n",
    "birthplaces_types['birthplace_types'] =  birthplaces_types['birthPlace_type1'] + ', ' + birthplaces_types['birthPlace_type2']\n",
    "birthplaces_types['birthplace_types'] = [text[2:] if text.startswith(', ') else text for text in birthplaces_types['birthplace_types']]\n",
    "\n",
    "# Merge the types\n",
    "data = data.merge(birthplaces_types, on=['pk_person', 'birthPlace'])\n",
    "\n",
    "# Select only the column we are interested in\n",
    "data = data[['pk_person', 'dbpedia_person_uri', 'wikidata_person_uri', 'pk_birth', 'birthPlace', 'birthplace_types', 'lat', 'lng']].drop_duplicates()\n",
    "\n",
    "# Drop duplicates to only have one record for each [pk_person, birthPlace]\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "a.infos(data, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a.infos(data, random=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse geographical places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_places = data[['birthPlace', 'birthplace_types', 'lat', 'lng']]\n",
    "geo_places.columns = ['name', 'types', 'lat', 'lng']\n",
    "\n",
    "# We aggregate all types available for a geo place name\n",
    "geo_places_types = geo_places.groupby('name')['types'].apply(lambda x: ','.join(sorted(list(dict.fromkeys((', '.join(x).split(', '))))))).reset_index()\n",
    "\n",
    "# We can now go back to initial geo_place date, and fill with aggregated types\n",
    "geo_places = geo_places.merge(geo_places_types, on='name').drop(columns=['types_x']).rename(columns={'types_y': 'types'})\n",
    "\n",
    "# We want only one row for each geo place\n",
    "geo_places.drop_duplicates(inplace=True)\n",
    "\n",
    "# We make sure that we only have one row for each place\n",
    "unicity = len(geo_places) == len(geo_places['name'].unique())\n",
    "print(f'Unicity: {unicity}, shape with duplicates: {geo_places.shape}')\n",
    "\n",
    "if not unicity:\n",
    "    gb = geo_places.groupby('name').count().reset_index()\n",
    "    names = gb[gb['types'] != 1]['name'].tolist()\n",
    "    print('Remaining duplicates:')\n",
    "    display(geo_places[geo_places['name'].isin(names)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that filter has been done, we observe that there are duplicates because some geographical places have multiple geo coordinates couples. \n",
    "\n",
    "So we just arbitrarily take one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_places.drop_duplicates(subset=['name'], inplace=True)\n",
    "\n",
    "u.infos(geo_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "geo_places.reset_index(inplace=True)\n",
    "geo_places['name'] = geo_places['name'].str.lower()\n",
    "geo_places['name'] = geo_places['name'].str.replace('_', ' ')\n",
    "geo_places['lat'] = geo_places['lat'].astype(float)\n",
    "geo_places['lng'] = geo_places['lng'].astype(float)\n",
    "\n",
    "# Record linkage\n",
    "matches = find.find_geoplaces(geo_places, 'index')\n",
    "\n",
    "matches.sort_values('new_name')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "- Comment on résoud le RL?\n",
    "- Pour les comparaisonss acceptée, si les coordonnées sont différentes, on ajoute des nouvelles ou on laisse les existantes?\n",
    "- Si on ajoute, on ajoute au niveau de la présence ou on ajoute une nouvelle présence?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
