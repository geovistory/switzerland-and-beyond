{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DB] Requests will not be executed\n",
      "[DB] Connecting to YELLOW database \"switzerland_and_beyond\" ... Connected!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_entities(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, requests, re\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "import geovpylib.database as db\n",
    "import geovdata.sparql as sparql\n",
    "import geovdata.kit as kit\n",
    "\n",
    "import spacy\n",
    "import scipy\n",
    "from pyvis.network import Network\n",
    "\n",
    "\n",
    "db.connect_yellow('switzerland_and_beyond')\n",
    "\n",
    "def ask_ollama(prompt, model='mistral'):\n",
    "    url='http://localhost:11434/api/generate'\n",
    "    response = requests.post(url, json={'model':model,'prompt':prompt, 'option':{'temperature':0}})\n",
    "    text = response.text.strip()\n",
    "    lines = text.split('\\n')\n",
    "    tokens = list(map(lambda line: json.loads(line)['response'], lines))\n",
    "    formated = ''.join(tokens)\n",
    "    answer = formated.strip()\n",
    "    return answer\n",
    "\n",
    "def analyze(string):\n",
    "    print(\"===== Original =====\")\n",
    "    print(string)\n",
    "    doc2 = nlp(string)\n",
    "    print(\"\\n===== Tokens =====\")\n",
    "    for token in doc2:\n",
    "        print(f\"{token.text} => LEMMA: {token.lemma_} |  POS: {token.pos_} | DEP: {token.dep_} | HEAD: {token.head}\")\n",
    "    print(\"\\n===== Entities =====\")\n",
    "    for ent in doc2.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(\"merge_entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naît le 26.10.1787 à Naples, meurt 1855, catholique, de Glaris et Ennenda (aujourd'hui commune de Glaris). Fils de Pasqual (https://hls-dhs-\n",
      "dss.ch/fr/articles/024356/2012-11-21/). Marié(e) (1822) à Maria Carolina, princesse de Liguoro, sans doute de la famille des marquis de Presicce.\n",
      "Prisonnier de guerre en France en 1812-1813. Dès 1821, Cajetan Tschudi s'efforça de recréer des régiments suisses à Naples, où il devint major de la\n",
      "Garde (1822), feld-maréchal et gouverneur de la ville de Naples (1824). Chargé d'affaires du royaume des Deux-Siciles à Berne (1832-1834), puis envoyé\n",
      "à Vienne et Constantinople, Cajetan Tschudi aurait reçu le titre de comte napolitain en 1846.\n"
     ]
    }
   ],
   "source": [
    "geoplaces_raw = db.query(f'select * from hls.person')\n",
    "\n",
    "geoplace = geoplaces_raw.sample(1).iloc[0]\n",
    "name = geoplace['name']\n",
    "notice = geoplace['notice']\n",
    "\n",
    "print(kit.wrap(notice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_response = ask_ollama(f\"\"\"\n",
    "# Provide me all statements you understand from the following text.\n",
    "# Statements should be short, concise like RDF triples, but in understandable text.\n",
    "# If dates are provided, they should have the following format: day.month.year.\n",
    "# Names should be fully written, each time.\n",
    "# Nothing should be implied.\n",
    "# Limit each statements: only one information should be in each statement.\n",
    "# Be exhaustive.\n",
    "                    \n",
    "# Text: \"{name}. {notice}\"\n",
    "# \"\"\")\n",
    "# # print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tschudi, Cajetan\n",
      "Cajetan Tschudi was born on 26.10.1787 in Naples\n",
      "Cajetan Tschudi died in 1855\n",
      "Cajetan Tschudi was Catholic\n",
      "Cajetan Tschudi was from Glaris and Ennenda (now a commune of Glaris)\n",
      "Pasqual is the father of Cajetan Tschudi\n",
      "Cajetan Tschudi married Maria Carolina, princess of Liguoro, in 1822\n",
      "Cajetan Tschudi was a prisoner of war in France from 1812 to 1813\n",
      "In 1821, Cajetan Tschudi tried to create Swiss regiments in Naples\n",
      "Cajetan Tschudi became major of the Guard in Naples in 1822\n",
      "Cajetan Tschudi was field-marshal and governor of Naples in 1824\n",
      "Cajetan Tschudi served as chargé d'affaires of the Two-Sicilian Kingdom in Bern from 1832 to 1834\n",
      "Cajetan Tschudi was sent to Vienna and Constantinople\n",
      "Cajetan Tschudi received the title of count napolitain in 1846.\n"
     ]
    }
   ],
   "source": [
    "llm_response = ask_ollama(f\"\"\"\n",
    "Provide me all statements you understand from the following text. \n",
    "Be exhaustive.\n",
    "Dates should have the following format: day.month.year.\n",
    "Your answer should be a list of short phrases about a single fact.\n",
    "Examples:\n",
    "- John Doe is the son of Martin\n",
    "- John Doe married Augustine in 1988\n",
    "- Augustine was a Catholic\n",
    "- Augustine's parents are Albert and Justine\n",
    "- Albert was a general\n",
    "                          \n",
    "Text: \" {name}. {notice}\"\n",
    "\"\"\")\n",
    "# print(llm_response)\n",
    "\n",
    "assertions = llm_response.split(\"\\n\")\n",
    "assertions = [text[text.find(' ') + 1:] for text in assertions]\n",
    "\n",
    "print(f'{name}')\n",
    "for assertion in assertions:\n",
    "    print(assertion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have a unique index\n",
    "index = 0\n",
    "def get_index():\n",
    "    global index\n",
    "    index += 1\n",
    "    return index\n",
    "\n",
    "\n",
    "# Keep knowledge of entity\n",
    "entities = []\n",
    "def get_entity(klass, label):\n",
    "    global entities\n",
    "\n",
    "    same = list(filter(lambda entity: entity['class'] == klass, entities))\n",
    "    same = list(filter(lambda entity: entity['identity'] == label, same))\n",
    "\n",
    "\n",
    "    if len(same) == 1: return same[0]\n",
    "    else: \n",
    "        to_return = {\n",
    "            'pk': get_index(),\n",
    "            'class': klass,\n",
    "            'identity': label,\n",
    "            'label': f\"{label.title()}\\n({klass})\"\n",
    "        }\n",
    "        entities.append(to_return)\n",
    "        return to_return\n",
    "\n",
    "\n",
    "def get_label_of_entity(pk):\n",
    "    global entities\n",
    "    same = list(filter(lambda entity: entity['pk'] == pk, entities))\n",
    "    return same[0]['label']\n",
    "\n",
    "colors = {\n",
    "    'Person': '#9c0ef3',\n",
    "    'Person Appellation in a Language': '#f3c169',\n",
    "    'Appellation in a Language': '#89e240',\n",
    "    'Birth':'#98922c',\n",
    "    'Death':'#6f034b',\n",
    "    'Religious Entity': '#e6586b',\n",
    "    'Geographical Place': '#f3c169',\n",
    "    'Union': '#83ea1d',\n",
    "    'Union Type': '#092524',\n",
    "}\n",
    "\n",
    "# '#a784a6', '#130551', '#4d0190', '#40b71b', '#215ef9', '#f9f357', '#586e11', '#e907bd', '#9f0a58', '#2b32f8', '#7a8e6b', '#436c81', '#dad258', '#eaaba4', '#8e5188', '#06889b', '#95b64f', '#7e99f6', '#e88b47', '#9e4a83', '#a117fe', '#5a5082', '#2a70e9', '#e56d08', '#a1fa9a', '#5d5e94', '#7ec00c', '#947763', '#741c90', '#fe53e2', '#0ce971', '#38c8ce', '#a4ef93', '#b450d6', '#c85099', '#81dc88', '#bc707a', '#a80cc6', '#266af4', '#6043b1', '#ee007c', '#e07f4b', '#e9f26d', '#9aa803', '#14ed04', '#55d40f', '#fab120', '#1c6b64', '#27c9ad', '#824e2a', '#b34149', '#96f684', '#e1ab51', '#635f12', '#5b95a7', '#1c5b22', '#55df5a', '#a47d61', '#9a491b', '#1f45f7', '#6ca29c', '#c5a715', '#a655f7', '#f74090', '#2577d5', '#9ee76f', '#22c01a', '#fabb7c', '#a4cdcf', '#9b0525', '#adf30f', '#1f738d', '#bb173f', '#3cc809', '#4f0110', '#76040d', '#f578f7', '#2bc21f', '#9597f7', '#8459d9', '#329cd0', '#9de477', '#4781fc', '#fecf42', '#e2b6ce', '#cfb109', '#364784', '#4532eb', '#941f7c', '#ab29a3', '#6ca99c', '#54a749', '#84230c', '#8bc063', '#67f846', '#4be798', '#013d0f', '#cf82a3', '#110404', '#e5c8d4', '#ee8a16', '#89460b', '#17672f', '#c19d44', '#b39921', '#f7438e', '#9324a1', '#82bfc6', '#b1a6e4', '#56513b', '#b3c2fb', '#2a434e', '#8e8cd1', '#91d5db', '#14ec07', '#14e7f7', '#bdea8b', '#73f303', '#d1c906', '#000321', '#1907c9', '#552b37', '#1499c5', '#4e0e58', '#f2d4a8', '#f44e30', '#e4d358', '#701166', '#168598', '#42650f', '#4b963b', '#b93479', '#5e1e17', '#b80582', '#5ca925', '#3d54c6', '#1e45f9', '#fd232d', '#134886', '#fe6739', '#1d1cfa', '#6924cc', '#b9c080', '#804e5b', '#0e3cb3', '#bbbbe3', '#168717', '#735890', '#abbca1', '#371551', '#975abc', '#6e5a3c', '#018b63', '#0143be', '#61a13a', '#e9f979', '#feaff5', '#0ba143', '#cc6a9c', '#7599d9', '#a0d008', '#5b27ac', '#719fb0', '#b669e0', '#4ac979', '#ad490a', '#ed68d5', '#e9887d', '#4031fb', '#f6156f', '#276755', '#6dcbce', '#60991d', '#507688', '#ca98f6', '#c6843e', '#4d4a25', '#8fea3c', '#a53ce6', '#1f16eb', '#ea126a', '#e20607', '#641c88', '#97fcc2', '#7c2d09', '#91211b', '#3018f0', '#5d504e', '#b5d205', '#bfbbb5', '#21397e', '#bb1e68', '#17b8aa', '#f8a8b3', '#4e8c73', '#9ac69e', '#58dd10', '#3e4b8a', '#fd2787', '#630a35', '#774fc9', '#e52839', '#d5b371', '#c0c0b9', '#950313', '#c73161', '#12604c', '#c6d14b', '#d6b6a8', '#db7c2d', '#2346fe', '#da8827', '#9c9f99', '#8d3d85', '#e40a2e', '#20bfb5', '#70472e', '#ec87aa', '#8bfa41', '#d1a5e7', '#541270', '#aa2663', '#14ac09', '#6f9cb2', '#2702df', '#2a426a', '#f78491', '#679d22', '#cfad3b', '#3023c8', '#a5f61e', '#0ba8fd', '#d3fca1', '#073c2d', '#b3643e', '#abce4f', '#4c4bba', '#6529fa', '#23228c', '#b4aced', '#776937', '#9dabc0', '#63f55b', '#415286', '#45137f', '#585fad', '#8d8d67', '#1c2c9f', '#0e47e8', '#8eb0b0', '#559884', '#3cb425', '#45bb4e', '#3686ef', '#1130ea', '#c3591c', '#2c39cc', '#b01232', '#2ab113', '#f391fe', '#d2cec5', '#9bf89f', '#051a42', '#ed6fcf', '#fe7fe5', '#c2925a', '#a1b0f1', '#7aaecb', '#681ea5', '#043aeb', '#df3f99', '#faec82', '#b798e0', '#1f828f', '#91bd83', '#cc9209', '#03afb5', '#3062e1', '#2045d1', '#e45bf6', '#8b59de', '#856fd8', '#c91b31', '#7b43a4', '#38a3c2', '#583c83', '#e0dad4', '#3d4db2', '#4aee3c', '#95fb65', '#9c840f', '#4ddc61', '#0924cd', '#fa9218', '#2d9a23', '#def281', '#3cda64', '#f74d97', '#da0382', '#636760', '#3a20cf', '#d5c4f0', '#61a8f7', '#d7e649', '#4ac80c', '#5b686e', '#7a8e1b', '#3a9ab0', '#9b97fc', '#ecdc25', '#266505', '#b6e4e4', '#dd5f76', '#77ad2c', '#9b87b0', '#e8077d', '#24c696', '#aeaf48', '#e1e1aa', '#188ef5', '#805e1c', '#945faa', '#4d0ed1', '#6ca92e', '#668501', '#d488b7', '#ade797', '#054625', '#8f3369', '#805f12', '#bf7680', '#bd9589', '#3127a5', '#3794ae', '#8eafed', '#61aae6', '#0be95b', '#fae52a', '#47c5bb', '#8382ab', '#632ab2', '#4873a4', '#970fbc', '#ec8e0d', '#a69e1a', '#98048b', '#035d30', '#7ccddf', '#be11f2', '#f1bf94', '#2b9a44', '#7cce82', '#9f90f8', '#3e1156', '#7b6857', '#daf5d5', '#6ee335', '#267878', '#67de78', '#236c54', '#3b1aec', '#551181', '#c5571c', '#dc10fc', '#52205e', '#41a05c', '#f4ba7a', '#c205cb', '#7c0779', '#a4d86b', '#5afb53', '#81fd52', '#8e70f9', '#34c715', '#43a329', '#797968', '#b51f34', '#721d62', '#cc73a9', '#dd00c0', '#819923', '#c382cc', '#3059de', '#dc94a6', '#cf89d2', '#a37ed2', '#8ea8dd', '#d53b4a', '#72a46f', '#4878f5', '#1c635c', '#8ca745', '#62d764', '#0c35b6', '#b92efe', '#4f144f', '#6507c0', '#920d32', '#113fc5', '#3c4fb1', '#dba26e', '#35ae7c', '#e8d2da', '#6d5670', '#35e603', '#19d3d6', '#06da43', '#284317', '#e1ed78', '#2df3de', '#441908', '#2642cb', '#86f7bd', '#714540', '#b7491e', '#f1e072', '#91e8c7', '#a93cb7', '#b7dbe9', '#015fc0', '#675b43', '#306842', '#b115ed', '#96e7b3', '#9f9527', '#38965f', '#3d0aec', '#5f86f9', '#f57005', '#f2fb2d', '#e9018a', '#183fef', '#b0c8ca', '#554076', '#5dcf35', '#09ec08', '#18b9c4', '#43e29d', '#77244b', '#fefb49', '#04ce9b', '#5e1e02', '#8b4448', '#e8a0e6', '#5d9782', '#86696c', '#089896', '#5be604', '#b033dc', '#85b2a8', '#7a5fd9', '#4cb070', '#d0fe22', '#d03e26', '#b0350b', '#302ffd', '#284a6c', '#f08c51', '#8053b7', '#65bab3', '#e269e4', '#4dbf13', '#5a2255', '#4dbc72', '#048779', '#02dc08', '#17b8b3', '#0f4808', '#40b729', '#483574', '#48ac05', '#78815e', '#21d0c3', '#1e227b', '#1fd4e8', '#1e6edc', '#147ad0', '#a94372', '#9ab7e5', '#bb61c9', '#29b91a', '#236d53', '#03793e', '#0df96a', '#fc9a1f', '#33ee69', '#3239f2', '#6236a4', '#bc340e', '#accbcd', '#59d166', '#8a1654', '#73a2c5', '#941645', '#68b8df', '#acf589', '#d6a82c', '#b99001', '#d9b5e6', '#1b463d', '#a166c0', '#b9f226', '#029c73', '#d595c7', '#6e25e0', '#99f3fb', '#5a8456', '#bc7ed4', '#944cdb', '#a3f078', '#b7310b', '#2dc5ee', '#175a47', '#564ce6', '#716a7c', '#2aef59', '#b64467', '#d8a568', '#8a8b15', '#f1fdaf', '#fe5c29', '#1eeeb7', '#65764a', '#3ce75f', '#64f2dc', '#cb39e2', '#d02ea4', '#a0856e', '#25195b', '#77ed69', '#89cad5', '#f930a2', '#b9404a', '#e8301d', '#86f63f', '#62e76d', '#c1415b', '#d50c9d', '#5008f0', '#e14954', '#2feb55', '#dde36e', '#3503af', '#8efcd7', '#2a75d9', '#c760b8', '#5554c3', '#2a4c13', '#ee9438', '#573a88', '#540a6c', '#433692', '#fcb514', '#7ec936', '#7a35a2', '#7cb539', '#8d9488', '#1d0ca3', '#175e4e', '#3fdb45', '#860218', '#6a4342', '#f1d774', '#3cc055', '#ec5de8', '#321485', '#34b40f', '#f0843d', '#cd43db', '#782a3b', '#0276d7', '#98a8f0', '#040a3d', '#dd9029', '#f3328c', '#5dd663', '#2cbd22', '#352d83', '#7392ef', '#e7020a', '#64f0c0', '#82f217', '#98fe02', '#27accd', '#299c32', '#829b74', '#351bb4', '#20d8ec', '#1de776', '#bc891e', '#9603af', '#1e551a', '#9dc3ce', '#205c0c', '#6ee829', '#a5b11c', '#d3644c', '#bd290e', '#d422f3', '#d2d6f8', '#5c42c0', '#fa5579', '#58c5df', '#033ff1', '#d37282', '#8b183b', '#097330', '#09b42e', '#a72f6f', '#636c35', '#0b0e5b', '#e6a275', '#0ce01b', '#ca5f7c', '#ee8a75', '#1903d6', '#238dfb', '#af1b68', '#7a35e6', '#0764f8', '#04c55d', '#c777a7', '#81a6b9', '#62c110', '#289d8a', '#fe79a0', '#69731a', '#522586', '#862c66', '#837781', '#5cb253', '#f8c7a6', '#b6b43c', '#b0d657', '#a44ce5', '#9b2635', '#218589', '#0c1567', '#23af0d', '#49dae7', '#855c03', '#fed3ab', '#43918d', '#6c2f2e', '#aa17b2', '#74187b', '#70c4ba', '#16052e', '#a39960', '#bd773b', '#14198b', '#5b2e34', '#d1190b', '#53023e', '#bce545', '#c7b9a6', '#6e384d', '#ac3cf5', '#6d830e', '#8a8db4', '#01f2d5', '#1e3788', '#2208a4', '#451de7', '#40421b', '#90e18f', '#69e223', '#3997c9', '#dca1dd', '#4f89e3', '#bace44', '#48ec43', '#fa49c2', '#e9a731', '#9651d9', '#48a5b7', '#a5715f', '#cf54fa', '#2887da', '#f9c1ae', '#90c694', '#4856f9', '#845289', '#eb14d0', '#003d85', '#562c38', '#ed6be2', '#d8c305', '#92c43c', '#b081dd', '#0bbc16', '#5b432e', '#149113', '#23fb21', '#3d055d', '#45d774', '#79cede', '#af82ec', '#ad4c58', '#f6392d', '#49fa48', '#c4a567', '#e9e080', '#9d5ed1', '#6efeae', '#e9f7b2', '#926f65', '#0fbbad', '#ce42cc', '#298132', '#4a9ec8', '#0b6e05', '#a49808', '#111cc3', '#f8f15f', '#62b96e', '#5f534e', '#215d1d', '#fc8bd4', '#6824f6', '#89750a', '#daee43', '#95c130', '#2379cf', '#67c311', '#3cd06a', '#2c41ca', '#c788e4', '#0af1c3', '#d08a25', '#77bfdd', '#ecf408', '#8cbdf2', '#cfce78', '#1e8510', '#039096', '#079c0b', '#184462', '#4ae113', '#c461be', '#0b6568', '#4b0816', '#6b8622', '#5216bc', '#64b9ba', '#a5d2d0', '#04998b', '#ae7354', '#326994', '#37e36c', '#abb1e8', '#d8c9d0', '#92aa49', '#8ad145', '#daadf7', '#cc6a03', '#3a263b', '#a3b12c', '#b66e1c', '#c5aa42', '#04f729', '#c94a38', '#a1a609', '#4cfc04', '#ade256', '#43fa50', '#c79f32', '#d746bb', '#ae27ee', '#fdc235', '#1cd202', '#843c2d', '#35420d', '#87be83', '#1de221', '#be0494', '#fe0087', '#f6f463', '#8b3943', '#08c6f1', '#8bac22', '#b800b7', '#db03b7', '#53000a', '#8f4d10', '#6c7e01', '#22a0fc', '#eec00e', '#28f011', '#31c864', '#527862', '#2fec05', '#974634', '#a50878', '#496030', '#3a587d', '#dd2102', '#63db29', '#321df8', '#dc8a56', '#f2a516', '#fb80e9', '#58f795', '#bf95d3', '#8c7bfe', '#28c146', '#67ab78', '#810cfd', '#191828', '#1b90a9', '#5b4c30', '#f134bd', '#39dbee', '#115fd1', '#e404bc', '#cc9ae3', '#6925da', '#265cf7', '#030b9e', '#f7b404', '#72dea6', '#2c4d02', '#6cee1a', '#ad3a7e', '#cd4377', '#834fa6', '#2f2e49', '#4ef729', '#b88f6c', '#1b5f4d', '#0dfea4', '#383503', '#c0f2a5', '#b0cd1f', '#b97658', '#7df5f5', '#6d099e', '#13f862', '#008913', '#5fa159', '#39393d', '#e38221', '#aba47f', '#ee2794', '#9b8794', '#4a9745', '#ddfc96', '#2182f0', '#9e4f90', '#a09884', '#f4c314', '#d43c31', '#fe8f83', '#b5596d', '#ae4fee', '#13987a', '#b109a8', '#c2cbb8', '#cfd8c9', '#ab46ad', '#b8c9a2', '#470e93', '#1b6251', '#87f033', '#d2af99', '#3633b0', '#9ab6bb', '#d24add', '#e7010b', '#35dfdc', '#564de3', '#638ef7', '#954632', '#e8e42b', '#c144dd', '#67fb58', '#a40836', '#f0bf54', '#e8b324', '#89f0fd', '#55d0d7', '#f3bf21', '#7fba3e', '#a82fbb', '#4cb66d', '#decdce', '#db3240', '#709b67', '#d21b86', '#4f9b96', '#8378b8', '#66f671', '#795795', '#9689e2', '#2187eb', '#d02f4d', '#858ec0', '#af75e5', '#44ee7c', '#73ddd1', '#f36c0d', '#7609e4', '#69777c', '#aa5b94', '#dd1225', '#6fc3eb', '#b8d6e2', '#008dee', '#a8dc2a', '#6404ea', '#38e945', '#142b74', '#81aed8', '#a5396b', '#b1daf1', '#259162', '#361a76', '#5bbf17', '#d531aa', '#c9dcd4', '#cc1473', '#cf3096', '#ad928e', '#23b26e', '#eb9e00', '#b19177', '#369921', '#cafce7', '#99514d', '#5dc1eb', '#358e66', '#055eab', '#546e1c', '#559267', '#cff019', '#efd11d', '#da5ad4', '#f9e680', '#d413dc', '#a9e570', '#16142e', '#65659c', '#ed51f8', '#ba56a9', '#b1dcee', '#646eb0', '#6c5c24', '#445ee4', '#e2f121', '#40c77b', '#2c6c15', '#111b3e', '#f7740b', '#41aa46', '#989f17', '#88c39a', '#0298cf', '#6baabe', '#2e85d4', '#68f2a6', '#bb1772', '#2bd959', '#35d78f', '#a09856', '#1f466f', '#808759', '#697959', '#331700', '#c14556', '#e932c5', '#e804cf', '#575840', '#706c37', '#49f149', '#b63f2a', '#3aa384', '#7de52f', '#f55a33', '#76745f', '#c14358', '#f5fd27', '#f854be', '#771311', '#70b407', '#a8d517', '#005c4d', '#809a5b', '#a60679', '#ed53d5', '#83f3f0', '#f5b2c8', '#3fb7ab', '#a5180e', '#285329', '#2f126c', '#3a3b4d', '#c070da', '#59d80f', '#cfe9cd', '#9e0045', '#ce819c', '#66ec1c', '#985b5f', '#5a1b86', '#817bd8', '#92d76e', '#6ea8d5', '#ca249b', '#65a1c9', '#91c02a', '#b6c4ae', '#6520af', '#578bee', '#00e4e7', '#578fb4', '#dc9714', '#fbd8e5', '#e8cb3a', '#388f2f', '#3a157a', '#37ab35', '#c378dd', '#d6ab5c', '#d4313c', '#041b4d', '#466ae7', '#43beac', '#2526d7', '#1415fa', '#b530ed', '#2893f9', '#70f52d', '#93a3bb', '#10c1f8', '#65e671', '#793f1a', '#9324aa', '#0551f7', '#3a72ca', '#647c47', '#a79d87', '#59e3ca', '#12d2c5', '#15182f', '#e93c3d', '#07fcd8', '#37015a', '#955e6a', '#4fee32', '#c4dc6b', '#c06273', '#7ad3e0', '#20e2cc', '#accec3', '#19b454', '#8db2bb', '#aaaf54', '#3807fb', '#851d85', '#5b835d', '#8dbab6', '#62e1e3', '#e5ebbc', '#a323c6', '#7318a6', '#a809b8', '#d8b318', '#93712a', '#1885f7', '#a27468', '#7976fa', '#49d350', '#4ced2a', '#0b6537', '#5807d6', '#b36431', '#19060f', '#8a095b', '#4817cb', '#b4225f', '#4776e8', '#8fa87f', '#24f37a', '#2aeb69', '#dc5e7c', '#b926f2', '#3ef2f1', '#04c40e', '#e2c458', '#3f9f62', '#a3e493', '#1e9577', '#05b168', '#aea050', '#b0d423', '#084679', '#7d1e0a', '#bd64d1', '#5cd11e', '#91bd5c', '#e86719', '#6fd258', '#547b88', '#6fa43e', '#73fce6', '#215537', '#2a8028', '#b05628', '#16910c', '#867cf7', '#3650c7', '#212c96', '#de7654', '#17f349', '#969763', '#727ffc', '#a5a831', '#60f8fd', '#eadf31', '#6a7953', '#da52ca', '#37ce9b', '#efea08', '#64e529', '#c9df79', '#585603', '#4b514a', '#827cb1', '#2353b6', '#82f493', '#3da26b', '#b79617', '#92379c', '#34feb1', '#8b401f', '#6a5441', '#f31dbe', '#bc1256', '#a83337', '#2fe8f9', '#94039f', '#06d816', '#2232d4', '#5c8801', '#ac1c83', '#a0f67d', '#204400', '#5b660c', '#a03e1d', '#c48bb0', '#d10f87', '#69ba1c', '#bb4aae', '#8491af', '#241790', '#b6ac59', '#de1936', '#3314f1', '#c717c2', '#ef36ac', '#47f62c', '#c6bbae', '#39117e', '#69a87b', '#140f34', '#e15ae6', '#05f561', '#eecbf4', '#fa5725', '#117f3c', '#0c47b6', '#f3f754', '#2cf89b', '#413599', '#9814d6', '#ee39bd', '#ae07e7', '#8a9fa8', '#c03247', '#fec207', '#6ce8c9', '#462b17', '#4bc1e7', '#837743', '#4c7921', '#a6f754', '#059d8b', '#330306', '#4dda12', '#9e9a99', '#418cef', '#34b09f', '#1bb4d4', '#850ce1', '#29a6ca', '#2231bd', '#369d44', '#f322e5', '#69e533', '#a2d02f', '#aca372', '#10b948', '#4eaeed', '#5fd9dc', '#bc4d0e', '#345664', '#756d82', '#75283b', '#74d1c9', '#0e5a9e', '#ad02fd', '#82f4d8', '#40801b', '#16258d', '#89a550', '#1f3fef', '#b3386a', '#2ab946', '#4398dc', '#7627d2', '#409927', '#d4db56', '#47a746', '#8b5459', '#c32bc3', '#7fd982', '#ec3f0f', '#c99ee2', '#253f67', '#adeef5', '#d80ed0', '#46a20e', '#a942ab', '#72d071', '#90a5fc', '#cf466e', '#3ff110', '#950128', '#0799f1', '#cc5773', '#603b7c', '#4aea85', '#78c78d', '#fe8b5f', '#5c762e', '#fcd9e0', '#451403', '#7576c2', '#c40c8a', '#dd01e5', '#dac938', '#b1bee8', '#8af464', '#978131', '#aa6745', '#e07083', '#b61e92', '#55a39f', '#1158bd', '#d61c62', '#f08f0f', '#bb9d10', '#1b136b', '#aeeeb2', '#2983cb', '#77bb9d', '#41db34', '#36c89a', '#61d04b', '#db5716', '#d62dd6', '#e23cbc', '#0eb60f', '#d02b18', '#c4f284', '#5cbd22', '#0d0a72', '#58af1b', '#1e9e08', '#2358a2', '#bee942', '#0dbf39', '#a64440', '#c955d4', '#dc102b', '#022992', '#13aad1', '#7b4d31', '#1c90a8', '#4f05b9', '#aa2154', '#7ff61b', '#781949', '#12a65a', '#c0012c', '#cd9a64', '#1a97de', '#82fe29', '#638765', '#80e200', '#12b469', '#21d758', '#da8889', '#2554d5', '#025fd6', '#78bd9d', '#791a9c', '#13a77d', '#e295f7', '#48e70e', '#b0cdb2', '#8bbeab', '#5a7a19', '#2229c3', '#986b6d', '#a37cb8', '#d20926', '#dd0378', '#82e0a7', '#60ed36', '#a3107e', '#d201c3', '#6680af', '#e07bc4', '#ebfbc9', '#93b77a', '#967dd1', '#6a07a8', '#07a996', '#706eef', '#a33b18', '#8f0e5a', '#dd068b', '#429878', '#30fa80', '#9cbb8e', '#e481c8', '#2dd10c', '#a83218', '#4361b9', '#96cbfe', '#d068d2', '#53d56a', '#f0e38c', '#f084fa', '#1c19d5', '#ec317d', '#b9fca8', '#2daf78', '#07990a', '#3f9caf', '#22694c', '#804255', '#67ee87', '#b96afa', '#4bf3da', '#557926', '#697152', '#4748ee', '#9759c5', '#907b71', '#7d6ac2', '#3a3a15', '#e752b9', '#083205', '#6464d9', '#afde8a', '#889198', '#8b2e47', '#34af0d', '#a23def', '#c8721c', '#7165d0', '#c5ddfc', '#8aa469', '#ca9988', '#77a31b', '#2e19fe', '#a9b699', '#604e1b', '#f1950e', '#b32222', '#85bbd1', '#5a7052', '#595177', '#50f0eb', '#dc00de', '#04c1fc', '#af2c51', '#c904d6', '#8535e8', '#095920', '#425e06', '#698dc7', '#fca60f', '#282def', '#1d0b74', '#f441a1', '#16816f', '#daf860', '#61e4d3', '#572f30', '#fdbd62', '#cefeda', '#fb0120', '#d68deb', '#80a6aa', '#d53a18', '#0f9501', '#ae5287', '#65f5e0', '#63d066', '#e38439', '#01efe8', '#d9bff0', '#e69612', '#69fd0d', '#4665fe', '#f5f9be', '#f2838a', '#aeef29', '#2876d2', '#41220f', '#91dc3a', '#cd990b', '#78a638', '#88f885', '#dbfc4b', '#c9fb32', '#316345', '#c7ee59', '#96d4fa', '#fc25e3', '#1280b0', '#e77a1f', '#61abc9', '#3e6a64', '#e9f426', '#29aa62', '#8c339a', '#9890af', '#624eb2', '#e9934a', '#2f7ea4', '#8cbb44', '#786223', '#6b0850', '#de267c', '#5f1910', '#c433f1', '#7faf19', '#19ae71', '#045fe6', '#5f0c7f', '#990e28', '#caea60', '#aae6d4', '#dae8e1', '#22378e', '#8ffdea', '#61ae6e', '#2879be', '#0b1ce1', '#46c02b', '#15145b', '#e1a658', '#095850', '#876b82', '#ec48df', '#bad7e4', '#0fa357', '#b42718', '#71ea6c', '#1f6cd4', '#540935', '#5c915a', '#39b7a0', '#460786', '#74d6d6', '#9a4d76', '#3b484d', '#b7741e', '#5c90f9', '#56a3b0', '#6fa353', '#995e81', '#3a585c', '#842c92', '#b68d83', '#e46a6c', '#dc8d49', '#cae1ae'\n",
    "\n",
    "def get_color_of_entity(pk):\n",
    "    global entities, colors\n",
    "    entity = list(filter(lambda entity: entity['pk'] == pk, entities))[0]\n",
    "    return colors[entity['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_persons_names(doc, verbose=False):\n",
    "\n",
    "    black_list = ['Glaris']\n",
    "    white_list = ['Adèle de Sellon']\n",
    "\n",
    "    persons = list(filter(lambda entity: entity.label_ == \"PERSON\", doc.ents))\n",
    "    persons = list(filter(lambda person: person.text not in black_list, persons))\n",
    "    persons = list(map(lambda person: person.text, persons))\n",
    "\n",
    "    for white_name in white_list:\n",
    "        if white_name in doc.text: \n",
    "            persons.append(white_name)\n",
    "\n",
    "    return persons\n",
    "\n",
    "def find_geoplace_names(doc, verbose=False):\n",
    "\n",
    "    black_list = []\n",
    "    white_list = ['Glaris', 'Zoug', 'Ennenda']\n",
    "\n",
    "    geoplaces = list(filter(lambda entity: entity.label_ == \"GPE\", doc.ents))\n",
    "    geoplaces = list(filter(lambda geoplace: geoplace.text not in black_list, geoplaces))\n",
    "    geoplaces = list(map(lambda geoplace: geoplace.text, geoplaces))\n",
    "\n",
    "    for white_name in white_list:\n",
    "        if white_name in doc.text: \n",
    "            geoplaces.append(white_name)\n",
    "\n",
    "    return geoplaces\n",
    "\n",
    "\n",
    "def find_religious_entity(doc, verbose=False):\n",
    "\n",
    "    black_list = ['sardinian', 'austrian', 'two-sicilian', 'swiss']\n",
    "    white_list = ['protestant', 'catholic']\n",
    "\n",
    "    religions = list(filter(lambda entity: entity.label_ == \"NORP\", doc.ents))\n",
    "    religions = list(filter(lambda religion: religion.text.lower() not in black_list, religions))\n",
    "    religions = list(map(lambda religion: religion.text.lower(), religions))\n",
    "\n",
    "    for white_name in white_list:\n",
    "        if white_name in doc.text.lower() and white_name not in religions: \n",
    "            religions.append(white_name)\n",
    "    \n",
    "    return religions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_number(number):\n",
    "    splitted = number.split('.')\n",
    "    if len(splitted) == 3: return (int(splitted[2]), int(splitted[1]), int(splitted[0].replace('th', '')))\n",
    "    print(number)\n",
    "    return int(number)\n",
    "\n",
    "def parse_date_string(date_str):\n",
    "    try:\n",
    "        date_str = date_str.replace('of ', '').replace(',', '').replace('the', '')\n",
    "        date_elements = date_str.split(' ')\n",
    "        day = pd.NA\n",
    "        month = pd.NA\n",
    "        for elmt in date_elements:\n",
    "            if 'january' in elmt.lower(): month = 1\n",
    "            elif 'february' in elmt.lower(): month = 2\n",
    "            elif 'march' in elmt.lower(): month = 3\n",
    "            elif 'april' in elmt.lower(): month = 4\n",
    "            elif 'may' in elmt.lower(): month = 5\n",
    "            elif 'june' in elmt.lower(): month = 6\n",
    "            elif 'july' in elmt.lower(): month = 7\n",
    "            elif 'august' in elmt.lower(): month = 8\n",
    "            elif 'september' in elmt.lower(): month = 9\n",
    "            elif 'october' in elmt.lower(): month = 10\n",
    "            elif 'november' in elmt.lower(): month = 11\n",
    "            elif 'december' in elmt.lower(): month = 12\n",
    "            elif 'st' in elmt or 'nd' in elmt or 'rd' in elmt or 'th' in elmt:\n",
    "                day = int(elmt.replace('st', '').replace('nd', '').replace('rd', '').replace('th', ''))\n",
    "            else: \n",
    "                year = int(elmt)\n",
    "        return (year, month, day)\n",
    "    except: pass\n",
    "    \n",
    "def find_dates(doc, verbose=False):\n",
    "    dates = list(filter(lambda entity: entity.label_ == \"DATE\", doc.ents))\n",
    "    dates = list(map(lambda date: parse_date_string(date.text), dates))\n",
    "\n",
    "    regex_dates = re.findall(r'\\b(?:0[1-9]|[12][0-9]|3[01])\\.(?:0[1-9]|1[0-2])\\.\\d{4}\\b', doc.text)\n",
    "    regex_dates = list(map(lambda date: parse_date_number(date), regex_dates))\n",
    "\n",
    "    all_dates = dates + regex_dates\n",
    "    return list(filter(lambda date: date != None, all_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Original =====\n",
      "Cajetan Tschudi died in 1855\n",
      "===== Tokens =====\n",
      "Cajetan Tschudi => LEMMA: Cajetan Tschudi |  POS: PROPN | DEP: nsubj | HEAD: died\n",
      "died => LEMMA: die |  POS: VERB | DEP: ROOT | HEAD: died\n",
      "in => LEMMA: in |  POS: ADP | DEP: prep | HEAD: died\n",
      "1855 => LEMMA: 1855 |  POS: NUM | DEP: pobj | HEAD: in\n",
      "\n",
      "===== Entities =====\n",
      "Cajetan Tschudi PERSON\n",
      "1855 DATE\n"
     ]
    }
   ],
   "source": [
    "analyze('Cajetan Tschudi died in 1855')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PERSONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_person(doc, verbose=False):\n",
    "\n",
    "    persons_raw = find_persons_names(doc, verbose)\n",
    "    persons = list(map(lambda person: get_entity('Person', person), persons_raw))\n",
    "\n",
    "    # Person name\n",
    "    for person in persons:\n",
    "        aial = get_entity('Person Appellation in a Language', person['identity'])\n",
    "        yield (aial['pk'], \"is appellation for language of\", person['pk'])\n",
    "        yield (aial['pk'], \"refers to name\", person['identity'])\n",
    "\n",
    "    # Origins\n",
    "    matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "    pattern_origins = [{'TEXT': 'from'}, {'ENT_TYPE': {'IN': ['GPE', 'ORG']}}, {'TEXT': 'and', 'OP': '?'}, {'ENT_TYPE': {'IN': ['GPE', 'ORG']}}]\n",
    "    matcher.add('ORIGIN', [pattern_origins])\n",
    "    matchings = matcher(doc)\n",
    "    spans = [spacy.tokens.Span(doc, start, end) for id, start, end in matchings]\n",
    "    for span in spans:\n",
    "        geoplaces = span.text.replace('from', '').split('and')\n",
    "        for geoplace_str in geoplaces:\n",
    "            for person in persons:\n",
    "                person = get_entity('Person', person['identity'])\n",
    "                geoplace = get_entity('Geographical Place', geoplace_str.strip())\n",
    "                yield (person['pk'], 'has its origins in', geoplace['pk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Original =====\n",
      "Cajetan Tschudi was from Glaris and Ennenda (now a commune of Glaris)\n",
      "===== Tokens =====\n",
      "Cajetan Tschudi => LEMMA: Cajetan Tschudi |  POS: PROPN | DEP: nsubj | HEAD: was\n",
      "was => LEMMA: be |  POS: AUX | DEP: ROOT | HEAD: was\n",
      "from => LEMMA: from |  POS: ADP | DEP: prep | HEAD: was\n",
      "Glaris => LEMMA: Glaris |  POS: PROPN | DEP: pobj | HEAD: from\n",
      "and => LEMMA: and |  POS: CCONJ | DEP: cc | HEAD: Glaris\n",
      "Ennenda => LEMMA: Ennenda |  POS: PROPN | DEP: conj | HEAD: Glaris\n",
      "( => LEMMA: ( |  POS: PUNCT | DEP: punct | HEAD: commune\n",
      "now => LEMMA: now |  POS: ADV | DEP: advmod | HEAD: commune\n",
      "a => LEMMA: a |  POS: DET | DEP: det | HEAD: commune\n",
      "commune => LEMMA: commune |  POS: NOUN | DEP: attr | HEAD: was\n",
      "of => LEMMA: of |  POS: ADP | DEP: prep | HEAD: commune\n",
      "Glaris => LEMMA: Glaris |  POS: PROPN | DEP: pobj | HEAD: of\n",
      ") => LEMMA: ) |  POS: PUNCT | DEP: punct | HEAD: was\n",
      "\n",
      "===== Entities =====\n",
      "Cajetan Tschudi PERSON\n",
      "Glaris ORG\n",
      "Ennenda ORG\n",
      "Glaris PERSON\n"
     ]
    }
   ],
   "source": [
    "analyze(\"Cajetan Tschudi was from Glaris and Ennenda (now a commune of Glaris)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GEOGRAPHICAL PLACE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_geoplace(doc, verbose=False):\n",
    "\n",
    "    geoplaces_raw = find_geoplace_names(doc, verbose)\n",
    "    geoplaces = list(map(lambda geoplace: get_entity('Geographical Place', geoplace), geoplaces_raw))\n",
    "\n",
    "    for geoplace in geoplaces:\n",
    "        aial = get_entity('Appellation in a Language', geoplace['identity'])\n",
    "        yield (aial['pk'], \"is appellation for language of\", geoplace['pk'])\n",
    "        yield (aial['pk'], \"refers to name\", geoplace['identity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BIRTHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_birth(doc, verbose=False):\n",
    "    \n",
    "    # Detetion parameters\n",
    "    exact_words = ['birth', 'born']\n",
    "    lemmas = ['died']\n",
    "\n",
    "    # Detection\n",
    "    extract = False\n",
    "    for token in doc:\n",
    "        if token.text.lower() in exact_words: extract = True\n",
    "        if token.lemma in lemmas: extract = True\n",
    "        if extract: break\n",
    "    if not extract: return []\n",
    "\n",
    "    # Extraction\n",
    "    if verbose: print(f'BIRTH')\n",
    "    persons = find_persons_names(doc, verbose)\n",
    "    dates = find_dates(doc, verbose)\n",
    "    geoplaces = find_geoplace_names(doc, verbose)\n",
    "\n",
    "    # Checks\n",
    "    if len(persons) > 1 or len(dates) > 1 or len(geoplaces) > 1:\n",
    "        print(\"-------\")\n",
    "        print(f\"Parsing BIRTH problem in :'{doc.text}'\")\n",
    "        print(\"--> Persons:\", persons)\n",
    "        print(\"--> Numbers:\", dates)\n",
    "        print(\"--> Geoplaces:\", geoplaces)\n",
    "        return []\n",
    "\n",
    "    person_name = persons[0] if len(persons) > 0 else 'Unknown person'\n",
    "    birth = get_entity('Birth', person_name)\n",
    "\n",
    "    # Extract persons\n",
    "    if len(persons) > 0: \n",
    "        if verbose: print(\"-- Person(s) found:\", persons)\n",
    "        person = get_entity('Person', persons[0])\n",
    "        yield (birth['pk'], \"brought into life\", person['pk'])\n",
    "\n",
    "    # Extract dates\n",
    "    if len(dates) > 0: \n",
    "        if verbose: print(\"-- Number(s) found:\", dates)\n",
    "        yield (birth['pk'], \"at some time within\", dates[0])\n",
    "\n",
    "    # Extract places\n",
    "    if len(geoplaces) > 0: \n",
    "        if verbose: print(\"-- GPE(s) found:\", geoplaces)\n",
    "        geoplace = get_entity('Geographical Place', geoplaces[0])\n",
    "        yield (birth['pk'], \"took place at\", geoplace['pk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEATH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_death(doc, verbose=False):\n",
    "    \n",
    "    # Detetion parameters\n",
    "    exact_words = ['death', 'died']\n",
    "    lemmas = ['died']\n",
    "\n",
    "    # Detection\n",
    "    extract = False\n",
    "    for token in doc:\n",
    "        if token.text.lower() in exact_words: extract = True\n",
    "        if token.lemma in lemmas: extract = True\n",
    "        if extract: break\n",
    "    if not extract: return []\n",
    "\n",
    "\n",
    "    # Extraction\n",
    "    if verbose: print(f'DEATH')\n",
    "    persons = find_persons_names(doc, verbose)\n",
    "    dates = find_dates(doc, verbose)\n",
    "    geoplaces = find_geoplace_names(doc, verbose)\n",
    "\n",
    "    # Checks\n",
    "    if len(persons) > 1 or len(dates) > 1 or len(geoplaces) > 1:\n",
    "        print(\"-------\")\n",
    "        print(f\"Parsing DEATH problem in :'{doc.text}'\")\n",
    "        print(\"--> Persons:\", persons)\n",
    "        print(\"--> Dates:\", dates)\n",
    "        print(\"--> Geoplaces:\", geoplaces)\n",
    "        return []\n",
    "\n",
    "    person_name = persons[0] if len(persons) > 0 else 'Unknown person'\n",
    "    death = get_entity('Death', person_name)\n",
    "\n",
    "    # Extract persons\n",
    "    if len(persons) > 0: \n",
    "        if verbose: print(\"-- Person(s) found:\", persons)\n",
    "        person = get_entity('Person', persons[0])\n",
    "        yield (death['pk'], \"was death of\", person['pk'])\n",
    "\n",
    "    # Extract dates\n",
    "    if len(dates) > 0: \n",
    "        if verbose: print(\"-- Number(s) found:\", dates)\n",
    "        yield (death['pk'], \"at some time within\", dates[0])\n",
    "\n",
    "    # Extract places\n",
    "    if len(geoplaces) > 0: \n",
    "        if verbose: print(\"-- GPE(s) found:\", geoplaces)\n",
    "        geoplace = get_entity('Geographical Place', geoplaces[0])\n",
    "        yield (death['pk'], \"took place at\", geoplace['pk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RELIGION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_religion(doc, verbose=False):\n",
    "\n",
    "    religions_raw = find_religious_entity(doc, verbose)\n",
    "    religions = list(map(lambda religion: get_entity('Religious Entity', religion), religions_raw))\n",
    "\n",
    "    for religion in religions:\n",
    "        aial = get_entity('Appellation in a Language', religion['identity'])\n",
    "        yield (aial['pk'], \"is appellation for language of\", religion['pk'])\n",
    "        yield (aial['pk'], \"refers to name\", religion['identity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONFESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_confession(doc, verbose=False):\n",
    "\n",
    "    # Get data\n",
    "    religions = find_religious_entity(doc, verbose)\n",
    "    persons = find_persons_names(doc, verbose)\n",
    "\n",
    "    # Verify minimal data\n",
    "    if len(religions) == 0 or len(persons) == 0:\n",
    "        return []\n",
    "\n",
    "    # Checks\n",
    "    if len(persons) > 1 or len(religions) > 1:\n",
    "        print(\"-------\")\n",
    "        print(f\"Problem in parsing document '{doc.text}' when extracting religions:\")\n",
    "        print(\"--> Persons:\", persons)\n",
    "        print(\"--> Religions:\", religions)\n",
    "        return []\n",
    "\n",
    "\n",
    "    confession = get_entity('Religious Entity', religions[0])\n",
    "    person = get_entity('Person', persons[0])\n",
    "\n",
    "    yield (confession['pk'], \"pertains to\", person['pk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PARENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_parents(doc, verbose=False):\n",
    "\n",
    "    # Detection parameters\n",
    "    words = ['son of', 'daughter of', 'parents', 'father', 'mother']\n",
    "\n",
    "    # Detection\n",
    "    extract = False\n",
    "    for words in words:\n",
    "        if words in doc.text: \n",
    "            extract = True\n",
    "    if not extract: return []\n",
    "\n",
    "    # Extraction\n",
    "    if verbose: print(f'PARENTS:', doc.text)\n",
    "    matcher = spacy.matcher.Matcher(nlp.vocab)\n",
    "    # pattern_child_1 = [{\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}, {'LEMMA': 'be'}]\n",
    "    pattern_child_2 = [{\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}, {'POS': 'PUNCT'}, {'LEMMA': {'IN': ['son', 'daughter']}}]\n",
    "    pattern_child_3 = [{\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}, {'LEMMA': \"'s\"}, {'LEMMA': {'IN': ['father', 'mother', 'parent']}}]\n",
    "    pattern_child_3 = [{\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}, {'LEMMA': \"'s\"}, {'LEMMA': {'IN': ['father', 'mother', 'parent']}}]\n",
    "    pattern_child_4 = [{'LEMMA': {'IN': ['father', 'mohter']}}, {'TEXT': 'of'}, {\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}]\n",
    "\n",
    "    pattern_parent_1 = [{'LEMMA': {'IN': ['son', 'daughter']}}, {'LEMMA':'of'}, {\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}]\n",
    "    pattern_parent_2 = [{'LEMMA':'and'}, {\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}]\n",
    "    pattern_parent_3 = [{'LEMMA': {'IN': ['father', 'mother']}}, {\"LEMMA\": \"be\"}, {\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}]\n",
    "    pattern_parent_4 = [{'LEMMA':'parent'}, {\"LEMMA\": \"be\"}, {\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}]\n",
    "    pattern_parent_5 = [{\"ENT_TYPE\": {\"IN\": [\"PERSON\", \"WORK_OF_ART\", \"ORG\"]}}, {'LEMMA':'be'}, {'TEXT': 'the'}, {'LEMMA': {'IN': ['father', 'mohter']}}]\n",
    "\n",
    "    # Search for the patterns\n",
    "    # matcher.add('CHILD_1', [pattern_child_1])\n",
    "    matcher.add('CHILD_2', [pattern_child_2])\n",
    "    matcher.add('CHILD_3', [pattern_child_3])\n",
    "    matcher.add('CHILD_4', [pattern_child_4])\n",
    "    matcher.add('PARENT_1', [pattern_parent_1])\n",
    "    matcher.add('PARENT_2', [pattern_parent_2])\n",
    "    matcher.add('PARENT_3', [pattern_parent_3])\n",
    "    matcher.add('PARENT_4', [pattern_parent_4])\n",
    "    matcher.add('PARENT_5', [pattern_parent_5])\n",
    "    matchings = matcher(doc)\n",
    "\n",
    "    # Extract persons\n",
    "    childs = []\n",
    "    parents = []\n",
    "    for id, start, end in matchings:\n",
    "        # if doc.vocab.strings[id] == 'CHILD_1': childs.append(str(doc[start:end-1]))\n",
    "        if doc.vocab.strings[id] == 'CHILD_2': childs.append(str(doc[start:end-2]))\n",
    "        if doc.vocab.strings[id] == 'CHILD_3': childs.append(str(doc[start:end-2]))\n",
    "        if doc.vocab.strings[id] == 'CHILD_4': childs.append(str(doc[start+2:end]))\n",
    "        if doc.vocab.strings[id] == 'PARENT_1': parents.append(str(doc[start+2:end]))\n",
    "        if doc.vocab.strings[id] == 'PARENT_2': parents.append(str(doc[start+1:end]))\n",
    "        if doc.vocab.strings[id] == 'PARENT_3': parents.append(str(doc[start+2:end]))\n",
    "        if doc.vocab.strings[id] == 'PARENT_4': parents.append(str(doc[start+2:end]))\n",
    "        if doc.vocab.strings[id] == 'PARENT_5': parents.append(str(doc[start:start+1]))\n",
    "\n",
    "    if verbose:\n",
    "        print('Childs:', childs)\n",
    "        print('Parents:', parents)\n",
    "\n",
    "    if len(childs) == 0 and len(parents) == 0:\n",
    "        return []\n",
    "\n",
    "    childs = [get_entity('Person', child) for child in childs]\n",
    "    births = [get_entity('Birth', child['identity']) for child in childs]\n",
    "    union = get_entity('Union', ' and '.join(parents))\n",
    "    parents = [get_entity('Person', parent) for parent in parents]\n",
    "\n",
    "    for parent in parents:\n",
    "        yield (union['pk'], 'has partner', parent['pk'])\n",
    "    \n",
    "    for i, birth in enumerate(births):\n",
    "        yield (birth['pk'], 'stemmed from', union['pk'])\n",
    "        yield (birth['pk'], 'brought into life', childs[i]['pk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MARRIAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_marriage(doc, verbose=False):\n",
    "\n",
    "    # Detection parameters\n",
    "    lemmas = ['marry']\n",
    "\n",
    "    # Detection\n",
    "    extract = False\n",
    "    for token in doc:\n",
    "        for lemma in lemmas:\n",
    "            if token.lemma_ == lemma:\n",
    "                extract = True\n",
    "    if not extract: return []\n",
    "\n",
    "    # Extraction (Persons)\n",
    "    if verbose: print(f'MARRIAGE')\n",
    "    married = []\n",
    "    for ent in doc.ents:\n",
    "        heads = [token.head.lemma_ for token in ent]\n",
    "        if ent.label_ == 'PERSON' and 'marry' in heads:\n",
    "            married.append(get_entity('Person', ent.text))\n",
    "    married_names = list(map(lambda person: person['identity'], married))\n",
    "    if verbose: print('Married:', married_names)\n",
    "\n",
    "    # Create the graph\n",
    "    union = get_entity('Union', ' and '.join(married_names))\n",
    "    union_type = get_entity('Union Type', 'Marriage')\n",
    "    yield (union['pk'], 'has type', union_type['pk'])\n",
    "    for person in married:\n",
    "        yield (union['pk'], 'has partner', person['pk'])\n",
    "\n",
    "    # Extraction (date)\n",
    "    dates = find_dates(doc, verbose)\n",
    "\n",
    "    if len(dates) > 1:\n",
    "        print(\"-------\")\n",
    "        print(f\"Parsing MARRIAGE problem in :'{doc.text}'\")\n",
    "        print(\"--> Dates:\", dates)\n",
    "        return []\n",
    "    \n",
    "    if len(dates) > 0: \n",
    "        yield (union['pk'], \"at some time within\", dates[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPUTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARENTS: Pasqual is the father of Cajetan Tschudi\n",
      "Childs: ['Cajetan Tschudi']\n",
      "Parents: ['Pasqual']\n"
     ]
    }
   ],
   "source": [
    "graph = []\n",
    "\n",
    "for doc in nlp.pipe(assertions):\n",
    "    graph += handle_person(doc, False)\n",
    "    graph += handle_geoplace(doc, False)\n",
    "    graph += handle_birth(doc, False)\n",
    "    graph += handle_death(doc, False)\n",
    "    graph += handle_religion(doc, False)\n",
    "    graph += handle_confession(doc, False)\n",
    "    graph += handle_parents(doc, True)\n",
    "    graph += handle_marriage(doc, False)\n",
    "\n",
    "for i, triplet in enumerate(graph):\n",
    "    if isinstance(triplet[2], str):\n",
    "        graph[i] = (graph[i][0], graph[i][1], graph[i][2].title())\n",
    "\n",
    "\n",
    "graph = pd.DataFrame(data=graph, columns=['subject', 'predicate', 'object'])\n",
    "graph.drop_duplicates(inplace=True)\n",
    "\n",
    "graph['subject_label'] = [get_label_of_entity(pk) for pk in graph['subject']]\n",
    "graph['object_label'] = [get_label_of_entity(pk) if isinstance(pk, int) else f\"{pk}\\n(Value)\" for pk in graph['object']]\n",
    "graph['subject_color'] = [get_color_of_entity(pk) if isinstance(pk, int) else f\"#000\" for pk in graph['subject']]\n",
    "graph['object_color'] = [get_color_of_entity(pk) if isinstance(pk, int) else f\"#000\" for pk in graph['object']]\n",
    "\n",
    "graph_small = graph[['subject_label', 'predicate', 'object_label']]\n",
    "graph_small.columns = ['subject', 'predicate', 'object']\n",
    "\n",
    "# graph_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph.html\n"
     ]
    }
   ],
   "source": [
    "nodes_subject = graph[['subject_label', 'subject_color']].drop_duplicates(subset=['subject_label'])\n",
    "nodes_object = graph[['object_label', 'object_color']].drop_duplicates(subset=['object_label'])\n",
    "\n",
    "network = Network(height=750, width=1500, notebook=True, cdn_resources='remote')\n",
    "# network = Network(height=750, width=1500, notebook=True, cdn_resources=\"in_line\", select_menu=True)\n",
    "network.add_nodes(nodes_subject['subject_label'].tolist(), color=nodes_subject['subject_color'].tolist())\n",
    "network.add_nodes(nodes_object['object_label'].tolist(), color=nodes_object['object_color'].tolist())\n",
    "\n",
    "for i, row in graph.iterrows():\n",
    "    network.add_edge(str(row['subject_label']), str(row['object_label']), label=row['predicate'])\n",
    "\n",
    "network.set_options(\"\"\"\n",
    "    const options = {\n",
    "        \"nodes\": {\"font\": {\"face\": \"tahoma\"}},\n",
    "        \"edges\": {\n",
    "            \"arrows\": {\"to\": {\"enabled\": true}},\n",
    "            \"font\": {\"size\": 10,\"face\": \"tahoma\",\"align\": \"top\"}\n",
    "        }\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "# network.show_buttons(filter_=['physics'])\n",
    "network.show('graph.html', local=True, notebook=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
