{"cells":[{"cell_type":"markdown","metadata":{},"source":["# HLS Web Scraping"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","[DB] Connecting to PGSQL Database ... Connected!\n"]}],"source":["# %load /home/gaetan/Desktop/geovpylib/templates/heading-admin.py\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Common imports\n","import os\n","import pandas as pd, numpy as np\n","import datetime\n","# import math\n","#import time\n","#import json\n","#import requests\n","#import duckdb\n","#import plotly.express as px\n","# from multiprocessing import Pool\n","\n","# Geovpylib library\n","import geovpylib.analysis as a\n","import geovpylib.database as db\n","import geovpylib.decorators as d\n","import geovpylib.importer as i\n","import geovpylib.magics\n","import geovpylib.pks as pks\n","import geovpylib.queries as q\n","import geovpylib.record_linkage as rl\n","import geovpylib.sparql as sparql\n","import geovpylib.utils as u\n","eta = u.Eta()\n","\n","# Specific imports\n","from selenium.webdriver import Chrome, ChromeOptions\n","from selenium.webdriver.common.by import By\n","\n","# Global variables\n","# ...\n","\n","# Connect to Geovistory('prod')\n","\n","# Connect to Geovistory database for insert\n","# env = 'prod' # Database to query: \"prod\", \"stag\", \"dev\", \"local\"\n","# pk_project = pks.projects. # The project to query/insert: integer\n","# execute = False # Boolean to prevent to execute directly into databases\n","# metadata_str = '' # kebab-lower-case or snake-lower-case. \n","# import_manner = 'one-shot' # 'one-shot' or 'batch'\n","# db.connect_geovistory(env, pk_project, execute)\n","# db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","# db.set_insert_manner(import_manner)\n","\n","# Connect to other database\n","db_url_env_var_name = 'YELLOW_SWITZERLAND_AND_BEYOND' # Name of an environment variable holding the Postgres database URL\n","execute = True # Boolean to prevent to execute directly into databases\n","db.connect_external(os.getenv(db_url_env_var_name), execute=execute)\n","\n","# Connect to a SPARQL endpoint\n","# sparql.connect_external('url')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Create tables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db.execute(\"\"\"\n","CREATE TABLE hls.theme (\n","\tid serial primary key,\n","\turl varchar,\n","\tname varchar,\n","\tnotice varchar,\n","\turi_geov varchar\n",");\n","CREATE TABLE hls.person (\n","\tid serial primary key,\n","\turl varchar,\n","\tname varchar,\n","\tnotice varchar,\n","\turi_geov varchar\n",");\n","CREATE TABLE hls.family (\n","\tid serial primary key,\n","\turl varchar,\n","\tname varchar,\n","\tnotice varchar,\n","\turi_geov varchar\n",");\n","CREATE TABLE hls.place (\n","\tid serial primary key,\n","\turl varchar,\n","\tname varchar,\n","\tnotice varchar,\n","\turi_geov varchar\n",");\n","\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["# Scrap all themes links"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_theme_links(browser, url):\n","    \"\"\"Load a page, extract name and URL, and write informations in database.\"\"\"\n","    \n","    browser.get(url)\n","    \n","    elements = browser.find_elements(By.CSS_SELECTOR, '.search-result')\n","    themes = db.query('select * from hls.theme')\n","    themes_set = set(themes['url'])\n","    \n","    sql = \"\"\n","    for elt in elements:\n","        link = elt.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n","        name = elt.find_element(By.CSS_SELECTOR, 'a > h2').text\n","        try:\n","            dates = elt.find_element(By.CSS_SELECTOR, 'a > h2 > span').text\n","        except Exception:\n","            dates = ''\n","\n","        link = link[0:link[:-1].rfind('/')] # remove the trailing date\n","        name = name.replace(dates, '').strip() # Remove the dates in the string\n","\n","        if link not in themes_set:\n","            sql += f\"\"\"\n","                insert into hls.theme \n","                    (url, name)\n","                    values ('{link}', '{name.replace(\"'\", \"''\")}');\n","            \"\"\"\n","    \n","    db.execute(sql)\n","\n","    if browser.find_elements(By.CSS_SELECTOR, '.noNextPagination'):\n","        return False\n","\n","    return browser.find_element(By.CSS_SELECTOR, '.nextPagination').get_attribute('href')\n","\n","\n","\n","browser = Chrome()\n","url = \"https://hls-dhs-dss.ch/fr/search/category?f_hls.lexicofacet_string=0%2F016900.&text=*&sort=score&sortOrder=desc&collapsed=true&r=1\"\n","cpt = 0\n","\n","eta.begin(158, 'Fetching themes urls')\n","while True:\n","    cpt += 1\n","    url = extract_theme_links(browser, url)\n","    if not url: break\n","    eta.iter()\n","eta.end()\n","\n","browser.quit()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Scrap all persons links"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_person_links(browser, url):\n","    \"\"\"Load a page, extract name and URL, and write informations in database.\"\"\"\n","    \n","    browser.get(url)\n","    \n","    elements = browser.find_elements(By.CSS_SELECTOR, '.search-result')\n","    persons = db.query('select * from hls.person')\n","    persons_set = set(persons['url'])\n","    \n","    sql = \"\"\n","    for elt in elements:\n","        link = elt.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n","        name = elt.find_element(By.CSS_SELECTOR, 'a > h2').text\n","        try:\n","            dates = elt.find_element(By.CSS_SELECTOR, 'a > h2 > span').text\n","        except Exception:\n","            dates = ''\n","\n","        link = link[0:link[:-1].rfind('/')] # remove the trailing date\n","        name = name.replace(dates, '').strip() # Remove the dates in the string\n","\n","        if link not in persons_set:\n","            sql += f\"\"\"\n","                insert into hls.person \n","                    (url, name)\n","                    values ('{link}', '{name.replace(\"'\", \"''\")}');\n","            \"\"\"\n","    \n","    db.execute(sql)\n","\n","    if browser.find_elements(By.CSS_SELECTOR, '.noNextPagination'):\n","        return False\n","\n","    return browser.find_element(By.CSS_SELECTOR, '.nextPagination').get_attribute('href')\n","\n","\n","\n","browser = Chrome()\n","url = \"https://hls-dhs-dss.ch/fr/search/category?f_hls.lexicofacet_string=0%2F000100.&text=*&sort=score&sortOrder=desc&collapsed=true&r=1\"\n","cpt = 0\n","\n","eta.begin(1268, 'Fetching persons urls')\n","while True:\n","    cpt += 1\n","    url = extract_person_links(browser, url)\n","    if not url: break\n","    eta.iter()\n","eta.end()\n","\n","browser.quit()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Scrap all families links"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_family_links(browser, url):\n","    \"\"\"Load a page, extract name and URL, and write informations in database.\"\"\"\n","    \n","    browser.get(url)\n","    \n","    elements = browser.find_elements(By.CSS_SELECTOR, '.search-result')\n","    families = db.query('select * from hls.family')\n","    families_set = set(families['url'])\n","    \n","    sql = \"\"\n","    for elt in elements:\n","        link = elt.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n","        name = elt.find_element(By.CSS_SELECTOR, 'a > h2').text\n","        try:\n","            dates = elt.find_element(By.CSS_SELECTOR, 'a > h2 > span').text\n","        except Exception:\n","            dates = ''\n","\n","        link = link[0:link[:-1].rfind('/')] # remove the trailing date\n","        name = name.replace(dates, '').strip() # Remove the dates in the string\n","\n","        if link not in families_set:\n","            sql += f\"\"\"\n","                insert into hls.family \n","                    (url, name)\n","                    values ('{link}', '{name.replace(\"'\", \"''\")}');\n","            \"\"\"\n","    \n","    db.execute(sql)\n","\n","    if browser.find_elements(By.CSS_SELECTOR, '.noNextPagination'):\n","        return False\n","\n","    return browser.find_element(By.CSS_SELECTOR, '.nextPagination').get_attribute('href')\n","\n","\n","\n","browser = Chrome()\n","url = \"https://hls-dhs-dss.ch/fr/search/category?f_hls.lexicofacet_string=0%2F000200.&text=*&sort=score&sortOrder=desc&collapsed=true&r=1\"\n","cpt = 0\n","\n","eta.begin(128, 'Fetching families urls')\n","while True:\n","    cpt += 1\n","    url = extract_family_links(browser, url)\n","    if not url: break\n","    eta.iter()\n","eta.end()\n","\n","browser.quit()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Scrap all places links"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_place_links(browser, url):\n","    \"\"\"Load a page, extract name and URL, and write informations in database.\"\"\"\n","    \n","    browser.get(url)\n","    \n","    elements = browser.find_elements(By.CSS_SELECTOR, '.search-result')\n","    places = db.query('select * from hls.place')\n","    places_set = set(places['url'])\n","    \n","    sql = \"\"\n","    for elt in elements:\n","        link = elt.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n","        name = elt.find_element(By.CSS_SELECTOR, 'a > h2').text\n","        try:\n","            dates = elt.find_element(By.CSS_SELECTOR, 'a > h2 > span').text\n","        except Exception:\n","            dates = ''\n","\n","        link = link[0:link[:-1].rfind('/')] # remove the trailing date\n","        name = name.replace(dates, '').strip() # Remove the dates in the string\n","\n","        if link not in places_set:\n","            sql += f\"\"\"\n","                insert into hls.place \n","                    (url, name)\n","                    values ('{link}', '{name.replace(\"'\", \"''\")}');\n","            \"\"\"\n","    \n","    db.execute(sql)\n","\n","    if browser.find_elements(By.CSS_SELECTOR, '.noNextPagination'):\n","        return False\n","\n","    return browser.find_element(By.CSS_SELECTOR, '.nextPagination').get_attribute('href')\n","\n","\n","\n","browser = Chrome()\n","url = \"https://hls-dhs-dss.ch/fr/search/category?f_hls.lexicofacet_string=0%2F006800.&text=*&sort=score&sortOrder=desc&collapsed=true&r=1\"\n","cpt = 0\n","\n","eta.begin(274, 'Fetching places urls')\n","while True:\n","    cpt += 1\n","    url = extract_place_links(browser, url)\n","    if not url: break\n","    eta.iter()\n","eta.end()\n","\n","browser.quit()"]},{"cell_type":"markdown","metadata":{},"source":["# Fetch notices for all themes"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Scraping themes notices is done - Elapsed: [00h00m00s]                                                                     \n"]}],"source":["browser = Chrome()\n","\n","themes = db.query('select * from hls.theme where notice is null')\n","\n","eta.begin(len(themes), 'Scraping themes notices')\n","for _, theme in themes.iterrows():\n","    if theme['notice']: \n","        eta.iter()\n","        continue\n","    \n","    browser.get(theme['url'])\n","    notice = browser.find_element(By.CSS_SELECTOR, '.hls-article-text-unit > p').text\n","\n","    db.execute(f\"\"\"\n","        update hls.theme\n","            set notice = '{notice.replace(\"'\", \"''\")}'\n","        where id = {theme['id']};\n","    \"\"\")\n","    eta.iter()\n","eta.end()\n","\n","browser.quit()"]},{"cell_type":"markdown","metadata":{},"source":["# Fetch notices for all persons"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Scraping persons notices is done - Elapsed: [00h31m12s]                                                                       \n"]}],"source":["browser = Chrome()\n","\n","persons = db.query('select * from hls.person where notice is null')\n","\n","eta.begin(len(persons), 'Scraping persons notices')\n","for _, person in persons.iterrows():\n","    if person['notice']: \n","        eta.iter()\n","        continue\n","    \n","    browser.get(person['url'])\n","    notice = browser.find_element(By.CSS_SELECTOR, '.hls-article-text-unit > p').text\n","\n","    try:\n","        birthdate = browser.find_element(By.CSS_SELECTOR, '.hls-article-text-unit > p > .hls-dnais').text\n","        if birthdate != '': notice = notice.replace(birthdate, 'Naît le ' + birthdate)\n","    except: pass\n","\n","    try:\n","        deathdate = browser.find_element(By.CSS_SELECTOR, '.hls-article-text-unit > p > .hls-ddec').text\n","        notice = notice.replace(deathdate, 'meurt le ' + birthdate)\n","    except: pass\n","\n","    db.execute(f\"\"\"\n","        update hls.person\n","            set notice = '{notice.replace(\"'\", \"''\")}'\n","        where id = {person['id']};\n","    \"\"\")\n","    eta.iter()\n","eta.end()\n","\n","browser.quit()"]},{"cell_type":"markdown","metadata":{},"source":["# Fetch notices for all families"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Scraping families notices is done - Elapsed: [00h00m00s]                                                                   \n"]}],"source":["browser = Chrome()\n","\n","families = db.query('select * from hls.family where notice is null')\n","\n","eta.begin(len(families), 'Scraping families notices')\n","for _, family in families.iterrows():\n","    if family['notice']: \n","        eta.iter()\n","        continue\n","    \n","    browser.get(family['url'])\n","    notice = browser.find_element(By.CSS_SELECTOR, '.hls-article-text-unit > p').text\n","\n","    db.execute(f\"\"\"\n","        update hls.family\n","            set notice = '{notice.replace(\"'\", \"''\")}'\n","        where id = {family['id']};\n","    \"\"\")\n","    \n","    eta.iter()\n","eta.end()\n","\n","browser.quit()"]},{"cell_type":"markdown","metadata":{},"source":["# Fetch notices for all places"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Scraping places notices is done - Elapsed: [00h26m47s]                                                                     \n"]}],"source":["browser = Chrome()\n","\n","places = db.query('select * from hls.place where notice is null')\n","\n","eta.begin(len(places), 'Scraping places notices')\n","for _, place in places.iterrows():\n","    if place['notice']: \n","        eta.iter()\n","        continue\n","    \n","    browser.get(place['url'])\n","    notice = browser.find_element(By.CSS_SELECTOR, '.hls-article-text-unit > p').text\n","\n","    db.execute(f\"\"\"\n","        update hls.place\n","            set notice = '{notice.replace(\"'\", \"''\")}'\n","        where id = {place['id']};\n","    \"\"\")\n","\n","    eta.iter()\n","eta.end()\n","\n","browser.quit()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
