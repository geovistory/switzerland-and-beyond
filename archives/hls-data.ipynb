{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[">> External SPARQL URL set to <https://query.wikidata.org/sparql>\n"]}],"source":["# %load /home/gaetan/Desktop/geovpylib/templates/heading.py\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Common imports\n","import os\n","import pandas as pd, numpy as np\n","import datetime\n","#import json\n","import requests\n","#import duckdb\n","#import plotly.express as px\n","\n","# Geovpylib library\n","import geovpylib.analysis as a\n","import geovpylib.database as db\n","import geovpylib.decorators as d\n","import geovpylib.magics\n","import geovpylib.pks as pks\n","import geovpylib.queries as q\n","import geovpylib.record_linkage as rl\n","import geovpylib.sparql as sparql\n","import geovpylib.utils as u\n","eta = u.Eta()\n","\n","# Specific imports\n","# ...\n","\n","# Global variables\n","# ...\n","\n","# Connect to Geovistory database\n","# env = 'prod' # Database to query: \"prod\", \"stag\", \"dev\", \"local\"\n","# pk_project = pks.projects. # The project to query/insert: integer\n","# execute = False # Boolean to prevent to execute directly into databases\n","# metadata_str = '' # kebab-lower-case or snake-lower-case. \n","# import_manner = 'one-shot' # 'one-shot' or 'batch'\n","# db.connect_geovistory(env, pk_project, execute)\n","# db.set_metadata({'import-id': datetime.datetime.today().strftime('%Y%m%d') + '-' + metadata_str})\n","# db.set_insert_manner(import_manner)\n","\n","# Connect to other database\n","# db_url_env_var_name = 'YELLOW-' # Name of an environment variable holding the Postgres database URL\n","# execute = False # Boolean to prevent to execute directly into databases\n","# db.connect_external(os.getenv(db_url_env_var_name), execute=False)\n","\n","# Connect to Wikidata SPARQL endpoint\n","sparql.connect_external('https://query.wikidata.org/sparql')"]},{"cell_type":"markdown","metadata":{},"source":["# Import HLS data"]},{"cell_type":"markdown","metadata":{},"source":["## 1/ Harvest data"]},{"cell_type":"markdown","metadata":{},"source":["### 1.1/ Wikidata"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hls_persons = sparql.query(\"\"\"\n","    SELECT ?hls_id ?uri_wikidataLabel ?uri_wikidata ?genderLabel ?birthdate ?birthplace ?deathdate ?deathplace ?description\n","    WHERE {\n","        ?uri_wikidata wdt:P31 wd:Q5 .\n","        ?uri_wikidata wdt:P902 ?hls_id .\n","        optional { ?uri_wikidata wdt:P21 ?gender . }\n","        optional { ?uri_wikidata wdt:P569 ?birthdate . }\n","        optional { ?uri_wikidata wdt:P19 ?birthplace . }\n","        optional { ?uri_wikidata wdt:P570 ?deathdate . }\n","        optional { ?uri_wikidata wdt:P20 ?deathplace . }\n","        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n","\n","        SERVICE wikibase:label { \n","            bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" . \n","            ?uri_wikidata schema:description ?description .\n","        }\n","    }\n","\"\"\")\n","                                \n","hls_persons.rename(columns={'uri_wikidataLabel': 'name', 'genderLabel': 'gender'}, inplace=True)\n","hls_persons['uris'] = 'https://hls-dhs-dss.ch/articles/' + hls_persons['hls_id'] + ' ; ' + hls_persons['uri_wikidata']\n","hls_persons = hls_persons[['name', 'gender', 'description', 'birthdate', 'birthplace', 'deathdate', 'deathplace', 'hls_id', 'uris']]\n","hls_persons['gender'] = hls_persons['gender'].replace('male', 'Male')\n","hls_persons['gender'] = hls_persons['gender'].replace('female', 'Female')\n","hls_persons['birthdate'] = [u.parse_date(strdate) for strdate in hls_persons['birthdate']]\n","hls_persons['deathdate'] = [u.parse_date(strdate) for strdate in hls_persons['deathdate']]\n","\n","a.infos(hls_persons, random=True)\n","\n","# 44s"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hls_birthplaces = sparql.query(\"\"\"\n","    SELECT ?place ?placeLabel ?coordplace\n","    WHERE {\n","        ?uri_wikidata wdt:P31 wd:Q5 .\n","        ?uri_wikidata wdt:P902 ?uri_hls .\n","        ?uri_wikidata wdt:P19 ?place .\n","        optional { ?place wdt:P625 ?coordplace . }\n","        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n","    }\n","\"\"\")                    \n","# a.infos(hls_birthplaces)  \n","\n","hls_deathplaces = sparql.query(\"\"\"\n","    SELECT ?place ?placeLabel ?coordplace\n","    WHERE {\n","        ?uri_wikidata wdt:P31 wd:Q5 .\n","        ?uri_wikidata wdt:P902 ?uri_hls .\n","        ?uri_wikidata wdt:P20 ?place .\n","        optional { ?place wdt:P625 ?coordplace . }\n","        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n","    }\n","\"\"\")\n","# a.infos(hls_deathplaces)\n","\n","hls_places = pd.concat([hls_birthplaces, hls_deathplaces]).drop_duplicates()\n","hls_places['lat'] = [point.replace('Point(', '').replace(')', '').split(' ')[1] if pd.notna(point) else pd.NA for point in hls_places['coordplace']]\n","hls_places['lng'] = [point.replace('Point(', '').replace(')', '').split(' ')[0] if pd.notna(point) else pd.NA for point in hls_places['coordplace']]\n","hls_places['lat'] = hls_places['lat'].astype(pd.Float64Dtype())\n","hls_places['lng'] = hls_places['lng'].astype(pd.Float64Dtype())\n","hls_places.rename(columns={'place':'uri', 'placeLabel':'name'}, inplace=True)\n","hls_places['kind'] = 'Settlement'\n","hls_places = hls_places[['name','kind','lat','lng','uri']]\n","a.infos(hls_places)\n","\n","# 14s"]},{"cell_type":"markdown","metadata":{},"source":["### 1.2/ Fetch additional persons URIs from `metagrid.ch`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_meta_grid_uris(name, clues):\n","    \"\"\"Given the name and some clues (to differentiate between entities that have the same name), fetch the other uris that metagrid has on an entity.\"\"\"\n","\n","    url = \"https://api.metagrid.ch/search\"\n","    response = requests.get(url, params={\"group\":1, \"query\":name}).json()\n","\n","    if 'concordances' not in response: return []\n","\n","    for concordance in response['concordances']:\n","        uris = []\n","        found = False\n","        for resource in concordance['resources']:\n","            uri = resource['link']['uri']\n","            clues_present = True\n","            for clue in clues:\n","                if uri.find(clue) == -1: clues_present = False\n","            if clues_present: found = True\n","            uris.append(uri)\n","        if found: return uris\n","\n","    return []\n","\n","# Test\n","# get_meta_grid_uris('Camille Guggenheim', ['hls-dhs-dss.ch', '043800'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%cache_it hls_persons\n","\n","# hls_persons = wikidata.copy()\n","hls_persons['hls_id'] = hls_persons['hls_id'].astype(str)\n","hls_persons['same_as'] = pd.NA\n","\n","eta.begin(len(hls_persons), 'Fetching additional URIs')\n","for i, row in hls_persons.iterrows():\n","    uris = get_meta_grid_uris(row['name'], ['hls-dhs-dss.ch', row['hls_id']])\n","    hls_persons.at[i, \"same_as\"] = \";\".join(uris)\n","    eta.iter()\n","eta.end()\n","\n","hls_persons.drop(columns=['uri_wikidata', 'uri_hls', 'hls_id'], inplace=True)\n","\n","a.infos(hls_persons)\n","\n","# 50min"]},{"cell_type":"markdown","metadata":{},"source":["### 1.3/ Geovistory data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["db.connect_geovistory('prod')"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["gv_persons = db.query(f\"\"\"\n","    select distinct\n","        r0.pk_entity as pk_person,\n","        a3.string as uri\n","    from information.resource r0 \n","    inner join projects.info_proj_rel ipr0 on ipr0.fk_entity = r0.pk_entity and ipr0.is_in_project = true\n","    inner join information.statement s1 on s1.fk_subject_info = r0.pk_entity and s1.fk_property = {pks.properties.entity_sameAsURI_URI}\n","    inner join projects.info_proj_rel ipr1 on ipr1.fk_entity = s1.pk_entity and ipr1.is_in_project = true\n","    inner join information.statement s2 on s2.fk_subject_info = s1.fk_object_info and s2.fk_property = {pks.properties.appe_hasValue_string}\n","    inner join projects.info_proj_rel ipr2 on ipr2.fk_entity = s2.pk_entity and ipr2.is_in_project = true\n","    inner join information.appellation a3 on a3.pk_entity = s2.fk_object_info\n","    where r0.fk_class = {pks.classes.person}                   \n","\"\"\")\n","\n","a.infos(gv_persons)\n","\n","# 17s"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["persons"]},{"cell_type":"markdown","metadata":{},"source":["## 2/ Record linkage"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1/ Persons"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a.infos(hls_persons[pd.notna(hls_persons.pk_gv)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%cache_it hls_persons\n","\n","hls_persons['pk_gv'] = pd.NA\n","\n","eta.begin(len(hls_persons), 'HLS/GV record linkage')\n","for i, row in hls_persons.iterrows():\n","    uris = row['same_as'].split(';')\n","    for uri in uris:\n","        selection = gv_persons[gv_persons['uri'] == uri]\n","        if len(selection) == 1:\n","            hls_persons.at[i, 'pk_gv'] = selection.iloc[0]['pk_person']\n","        if len(selection) > 1: \n","            eta.print(f'Multiple matches found for uri {uri}')\n","    eta.iter()\n","eta.end()"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2/ Geographical places"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["q.find"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rl.find_geo_places(hls_places)"]},{"cell_type":"markdown","metadata":{},"source":["## 3/ Work with the geographical places"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["geo_places = hls_persons['birthplace'].dropna().tolist() + hls_persons['deathplace'].dropna().tolist()\n","geo_places = np.unique(geo_places)\n","geo_places = [uri[uri.rindex('/') + 1:] for uri in geo_places]\n","geo_places = list(filter(lambda uri: uri.startswith('Q'), geo_places))\n","\n","print('Geographical places number:', len(geo_places))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filter_str = '?s = wd:' + ' || ?s = wd:'.join(geo_places[:100])\n","# filter_str = '?s = wd:Q48958 || ?s = wd:Q100123'\n","\n","wikidata_geo_places = sparql.query(\"\"\"\n","    SELECT ?s ?sLabel ?coord\n","    WHERE {\n","        ?s wdt:P625 ?coord .\n","                                   \n","        filter(\"\"\" + filter_str + \"\"\")\n","\n","        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n","    }\n","\"\"\")\n","wikidata_geo_places.rename(columns={'sLabel': 'name'}, inplace=True)\n","wikidata_geo_places['lat'] = [point.replace('Point(', '').replace(')', '').split(' ')[1] for point in wikidata_geo_places['coord']]\n","wikidata_geo_places['lng'] = [point.replace('Point(', '').replace(')', '').split(' ')[0] for point in wikidata_geo_places['coord']]\n","wikidata_geo_places.drop(columns=['coord'], inplace=True)\n","\n","wikidata_geo_places"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
